{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic-Modelling\n",
    "\n",
    "## What is Topic Modelling?\n",
    "* a method for unsupervised classification of documents, similar to clustering on numeric data, which finds some natural groups of items (topics) even when we’re not sure what we’re looking for.\n",
    "* A document can be a part of multiple topics, kind of like in fuzzy clustering(soft clustering) in which each data point belongs to more than one cluster.\n",
    "\n",
    "## Why Topic Modelling?\n",
    "Topic modeling provides methods for automatically organizing, understanding, searching, and summarizing large electronic archives.\n",
    "It can help with the following:\n",
    "* discovering the hidden themes in the collection.\n",
    "* classifying the documents into the discovered themes.\n",
    "* using the classification to organize/summarize/search the documents.\n",
    "\n",
    "For example, let’s say a document belongs to the topics food, dogs and health. So if a user queries “dog food”, they might find the above-mentioned document relevant because it covers those topics(among other topics). We are able to figure its relevance with respect to the query without even going through the entire document.\n",
    "Therefore, by annotating the document, based on the topics predicted by the modeling method, we are able to optimize our search process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "from IPython.display import display\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main.ipynb\n",
      "pdfToText.py\n",
      "normalSimilarity.py\n",
      ".0\n",
      "a.txt\n",
      "abcnews-date-text.csv\n",
      ".ipynb_checkpoints\n",
      "__pycache__\n",
      "b.txt\n",
      "abcnews-date-text.csv.zip\n"
     ]
    }
   ],
   "source": [
    "for f in os.listdir(os.getcwd()):\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-Extraction\n",
    "\n",
    "Extract data from csv file as a dataframe object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>20030219</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>20030219</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>20030219</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date                                      headline_text\n",
       "0      20030219  aba decides against community broadcasting lic...\n",
       "1      20030219     act fire witnesses must be aware of defamation\n",
       "2      20030219     a g calls for infrastructure protection summit\n",
       "3      20030219           air nz staff in aust strike for pay rise\n",
       "4      20030219      air nz strike to affect australian travellers"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1186013</td>\n",
       "      <td>20191231</td>\n",
       "      <td>vision of flames approaching corryong in victoria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1186014</td>\n",
       "      <td>20191231</td>\n",
       "      <td>wa police and government backflip on drug amne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1186015</td>\n",
       "      <td>20191231</td>\n",
       "      <td>we have fears for their safety: victorian premier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1186016</td>\n",
       "      <td>20191231</td>\n",
       "      <td>when do the 20s start</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1186017</td>\n",
       "      <td>20191231</td>\n",
       "      <td>yarraville shooting woman dead man critically ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         publish_date                                      headline_text\n",
       "1186013      20191231  vision of flames approaching corryong in victoria\n",
       "1186014      20191231  wa police and government backflip on drug amne...\n",
       "1186015      20191231  we have fears for their safety: victorian premier\n",
       "1186016      20191231                              when do the 20s start\n",
       "1186017      20191231  yarraville shooting woman dead man critically ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of entries are 1186018\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('abcnews-date-text.csv')\n",
    "display(data.head())\n",
    "display(data.tail())\n",
    "print(\"Total number of entries are\", len(data.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text  index\n",
       "0  aba decides against community broadcasting lic...      0\n",
       "1     act fire witnesses must be aware of defamation      1\n",
       "2     a g calls for infrastructure protection summit      2\n",
       "3           air nz staff in aust strike for pay rise      3\n",
       "4      air nz strike to affect australian travellers      4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_text = data[['headline_text']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text\n",
    "display(documents.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "1. **Tokenisation** : Split sentences into words(tokens)\n",
    "2. Stop-words removal\n",
    "3. remove all words with length < 3.\n",
    "4. **Lemmatize** words: words in third person are changed to first person and verbs in past and future tenses are changed into present.\n",
    "5. **Stemming** : words are reduced to their root form.\n",
    "\n",
    "# Stemming\n",
    "\n",
    "bringing a given word down to its root word, eliminating any information of tense it might be holding\n",
    "* for instance, waiting-past tense of *wait*, hence the *stem* word is *wait*\n",
    "* beautiful $\\rightarrow$ beauty, hence spelling may also change while *stemming* a word\n",
    "\n",
    "The main aim is to reduce the **inflectional** forms of each word into a common base word or root word or stem word.\n",
    "Inflection is a process of word formation, in which a word is modified to express different grammatical categories such as tense, case, voice, aspect, person, number, gender, mood, animacy, and definiteness.\\\n",
    "\n",
    "2 kinds of stemming mainly is done:\n",
    "1. Over-Stemming\n",
    "2. Under-Stemming\n",
    "\n",
    "## Over-Stemming\n",
    "Two words with different stems are stemmed to the same root. This is also known as a false positive.\\\n",
    "* universal\n",
    "* university\n",
    "* universe\n",
    "\n",
    "All the above 3 words are stemmed to **univers**, which is wrong behavior.\n",
    "Though these three words are etymologically related, their modern meanings are in widely different domains, so treating them as synonyms in NLP/NLU will likely reduce the relevance of the search results\n",
    "\n",
    "## Under-Stemming\n",
    "Two words that should be stemmed to the same root are not. This is also known as a false negative.\n",
    "* alumnus\n",
    "* alumni\n",
    "* alumnae\n",
    "\n",
    "## SnowballStemmer\n",
    "\n",
    "\n",
    "# Lemmatization\n",
    "\n",
    "describe what is lemmatizing words here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/laferrari/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "\n",
    "# we need internet connection for this\n",
    "# check in your ~/nltk_data/corpora for wordnet.zip\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in simple_preprocess(text):\n",
    "        if len(token) > 3 and token not in STOPWORDS:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['ratepayers', 'group', 'wants', 'compulsory', 'local', 'govt', 'voting']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['ratepay', 'group', 'want', 'compulsori', 'local', 'govt', 'vote']\n"
     ]
    }
   ],
   "source": [
    "# pick some random entry from the headlines dataframe object\n",
    "doc_sample = documents[documents['index'] == 4310].values[0][0]\n",
    "print('original document: ')\n",
    "words = nltk.word_tokenize(doc_sample)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time of preprocessing was 0:02:40.492712 s\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     [decid, communiti, broadcast, licenc]\n",
       "1                        [wit, awar, defam]\n",
       "2    [call, infrastructur, protect, summit]\n",
       "3               [staff, aust, strike, rise]\n",
       "4      [strike, affect, australian, travel]\n",
       "Name: headline_text, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tic1 = datetime.now()\n",
    "processed_docs = documents['headline_text'].map(preprocess)\n",
    "tot_time = datetime.now() - tic1\n",
    "print(\"Total time of preprocessing was\", tot_time,\"s\\n\\n\\n\")\n",
    "display(processed_docs.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 broadcast\n",
      "1 communiti\n",
      "2 decid\n",
      "3 licenc\n",
      "4 awar\n",
      "5 defam\n",
      "6 wit\n",
      "7 call\n",
      "8 infrastructur\n",
      "9 protect\n",
      "10 summit\n",
      "total time required for this cell is, 0:00:15.246656\n"
     ]
    }
   ],
   "source": [
    "tic1 = datetime.now()\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break\n",
    "\n",
    "tot_time = datetime.now()-tic1\n",
    "print(\"total time required for this cell is,\", tot_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now proceed to filter out those words that dont occur in half of the documents, i.e. half of the processed headlines, and those that occur in less than 15 total documents.\\\n",
    "After this, we keep only the top 100,000 most frequent words/tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert each document to a bag-of-words, where we have an idea of each word's count in that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(162, 1), (240, 1), (292, 1), (589, 1), (838, 1), (3567, 1), (3568, 1)]\n"
     ]
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "print(bow_corpus[4310])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 162 (\"govt\") appears 1 time(s).\n",
      "Word 240 (\"group\") appears 1 time(s).\n",
      "Word 292 (\"vote\") appears 1 time(s).\n",
      "Word 589 (\"local\") appears 1 time(s).\n",
      "Word 838 (\"want\") appears 1 time(s).\n",
      "Word 3567 (\"compulsori\") appears 1 time(s).\n",
      "Word 3568 (\"ratepay\") appears 1 time(s).\n"
     ]
    }
   ],
   "source": [
    "bow_doc_4310 = bow_corpus[4310]\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time(s).\".format(bow_doc_4310[i][0], \n",
    "                                               dictionary[bow_doc_4310[i][0]], \n",
    "bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF model\n",
    "\n",
    "## Term Frequency (TF)\n",
    "1. The number of times a word appears in a document divded by the total number of words in the document. \n",
    "2. Every document has its own term frequency.\n",
    "\n",
    "## Inverse Data Frequency (IDF)\n",
    "1. The log of the number of documents divided by the number of documents that contain the word w. \n",
    "2. Inverse data frequency determines the weight of rare words across all documents in the corpus.\n",
    "3. IDF is a measure of how common any particular word or gram is in the given corpus that you are searching. It is an estimate of how rare that word is and thus its likely importance. So if a query contains an uncommon word, documents containing that rare word should be judged to be more important.\n",
    "\n",
    "TFIDF is simply their product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the TFIDF corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5850076620505259),\n",
      " (1, 0.38947256567331934),\n",
      " (2, 0.4997099083387053),\n",
      " (3, 0.5063271308533074)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "from pprint import pprint\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">bow_corpus</font>: (word-id, word-count in that document)\\\n",
    "<font color=\"green\">corpus_tfidf</font>: (word-id, tfidf value of that word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA-Latent Dirichlet Allocation\n",
    "[research paper link](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)\n",
    "* document = collection of words\n",
    "* topic = also a collection of words\n",
    "\n",
    "## Finding Representative Words for a Topic\n",
    "* We can sort the words with respect to their probability score. The top x words are chosen from each topic to represent the topic. If x = 10, we’ll sort all the words in topic1 based on their score and take the top 10 words to represent the topic. This step may not always be necessary because if the corpus is small we can store all the words in sorted by their score.\n",
    "* Alternatively, we can set a threshold on the score. All the words in a topic having a score above the threshold can be stored as its representative, in order of their scores.\n",
    "\n",
    "<font color=\"maroon\" size=\"5\">Assumptions</font>\n",
    "* Each document is just a collection of words or a “bag of words”. Thus, the order of the words and the grammatical role of the words (subject, object, verbs, …) are not considered in the model.\n",
    "* Words like am/is/are/of/a/the/but/… don’t carry any information about the “topics” and therefore can be eliminated from the documents as a preprocessing step. In fact, we can eliminate words that occur in at least %80 ~ %90 of the documents, without losing any information. For example, if our corpus contains only medical documents, words like human, body, health, etc might be present in most of the documents and hence can be removed as they don’t add any specific information which would make the document stand out.\n",
    "* We know beforehand how many topics we want. ‘k’ is pre-decided.\n",
    "* All topic assignments except for the current word in question are correct, and then updating the assignment of the current word using our model of how documents are generated\n",
    "\n",
    "The Algorithm is used to calculate the probability of words that belong to a topic.\n",
    "\n",
    "## Steps\n",
    "* Go through each document and randomly assign each word in the document to one of k topics (k is chosen beforehand).\n",
    "* For each document d, go through each word w and compute :\n",
    "    * p(topic t | document d): \n",
    "        * the proportion of words in document d that are assigned to topic t. \n",
    "        * Tries to capture how many words belong to the topic t for a given document d. \n",
    "        * <font color=red>Excluding the current word</font>. If a lot of words from d belongs to t, it is more probable that word w belongs to t.\n",
    "    * p(word w| topic t): \n",
    "        * the proportion of assignments to topic t over all documents that come from this word w. \n",
    "        * Tries to capture how many documents are in topic t because of word w. \n",
    "        * LDA represents documents as a mixture of topics. Similarly, a topic is a mixture of words. If a word has high probability of being in a topic, all the documents having w will be more strongly associated with t as well. Similarly, if w is not very probable to be in t, the documents which contain the w will be having very low probability of being in t, because rest of the words in d will belong to some other topic and hence d will have a higher probability for those topic. So even if w gets added to t, it won’t be bringing many such documents to t.\n",
    "  \n",
    "* Update the probability for the word w belonging to topic t, as: p(word with topic t) = p(topic t | document d) $\\times$p(word w | topic t)\n",
    "\n",
    "## Example\n",
    "Following is an example to demonstrate the steps enumerated above.\n",
    "1. Suppose you have various photographs(documents) with captions(words). You want to display them in a gallery so you decide to categorize the photographs on various themes(topics) based on which you will create different sections in your gallery.\n",
    "2. suppose you decide to create k=2 sections in your album — nature and city.The classification isn’t so clear as some photographs with city have trees and flowers while the nature ones might have some buildings in it. \n",
    "3. You decide to assign the photographs which only have nature or city elements in them into their respective categories while you randomly assigned the rest.\n",
    "4. You notice that a lot of photographs in nature have the word tree in their captions. So you concluded that the word tree and topic nature must be closely related.\n",
    "5. Next, you pick the word building and check how many photographs are in nature because they have the word building in their caption. You don’t find many and now are less sure about building belonging to the topic nature and associate it more strongly with the topic city.\n",
    "6. You then pick a photograph which has the caption “The tree is in front of the building and behind a car” and see that it is in the category nature currently.\n",
    "    1. You then chose the word tree, and calculate the first probability p(topic t | document d): other words in the caption are building and car, most photographs having captions with building or car in it are in city, so you get a low probability.\n",
    "    2. Now,for the second probability p(word w | topic t): we know that a lot of photographs in nature have the word trees in it. So you get a high score here.\n",
    "    3. You update the probability of tree belonging in nature by multiplying the two. You get a lower value than before for *tree* in **topic nature** because now you have seen that tree and words such as building/car in the same caption, implying that *trees can also be found in cities*.\n",
    "    4. For the same reason, when you update the probability for tree belonging in topic city, you will notice that it will be greater than what it was before.(initially, since tree was assigned nature, this means that p(w=tree|t=city) = 1-p(w=tree|t=nature), will increase, and it initially had a very low value.\n",
    "7. After multiple iterations over all the photographs and for each topic, you will have accurate scores for each word with respect to each topic. \n",
    "8. Your guesses will keep getting better and better because you’ll conclude from the context that words such as buildings, sidewalk, subway appear together and hence must belong to the same topic, which we can easily guess is city.\n",
    "9. Words such as mountains, fields, beach which might not appear together in a lot of captions but they do appear often without city words and hence will have higher scores for nature.\n",
    "10. While words such as trees, flowers, dogs, sky will have almost the same probability of being in either as they occur in both topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating LDA models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## choose the best model\n",
    "* instantiate models for different k-values, and plot the coherence scores.\n",
    "* If the coherence score seems to keep increasing, it may make better sense to pick the model that gave the highest CV before flattening out. This is exactly the case here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with k=2 was instanced on 2020-06-01 17:15:33.579096\n",
      "time to complete model with k=2 was 0:05:49.056046\n",
      "Model with k=8 was instanced on 2020-06-01 17:21:34.087284\n",
      "time to complete model with k=8 was 0:05:32.669257\n",
      "Model with k=14 was instanced on 2020-06-01 17:27:32.607774\n",
      "time to complete model with k=14 was 0:05:19.595673\n",
      "Model with k=20 was instanced on 2020-06-01 17:33:33.833339\n",
      "time to complete model with k=20 was 0:05:08.897460\n",
      "Model with k=26 was instanced on 2020-06-01 17:39:33.321191\n",
      "time to complete model with k=26 was 0:04:53.147767\n",
      "Model with k=32 was instanced on 2020-06-01 17:45:26.931020\n",
      "time to complete model with k=32 was 0:04:42.840440\n",
      "Model with k=38 was instanced on 2020-06-01 17:51:10.116624\n",
      "time to complete model with k=38 was 0:04:55.929103\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaMulticore, CoherenceModel\n",
    "time_list, coherence_scores = [], {}\n",
    "for k in range(2, 40, 6):\n",
    "    tic1 = datetime.now()\n",
    "    print(\"Model with k={} was instanced on {}\".format(k, tic1))\n",
    "    lda_model = LdaMulticore(bow_corpus, num_topics=k, id2word=dictionary, passes=2, workers=2)\n",
    "    tot_time = datetime.now()-tic1\n",
    "    print(\"time to complete model with k={} was {}\".format(k, tot_time))\n",
    "    time_list.append(tot_time)\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_docs, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    coherence_scores[k] = coherence_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(list(coherence_scores.keys()), list(coherence_scores.values()), c='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time of training the LDA model is 0:05:19.470929\n"
     ]
    }
   ],
   "source": [
    "tic1 = datetime.now()\n",
    "lda_model = LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)\n",
    "tot_time = datetime.now()-tic1\n",
    "print(\"Total time of training the LDA model is\", tot_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      " Words: 0.022*\"hous\" + 0.021*\"south\" + 0.019*\"canberra\" + 0.019*\"north\" + 0.016*\"bushfir\" + 0.016*\"miss\" + 0.013*\"interview\" + 0.011*\"hospit\" + 0.010*\"investig\" + 0.010*\"search\"\n",
      "Topic: 1 \n",
      " Words: 0.031*\"kill\" + 0.023*\"shoot\" + 0.021*\"dead\" + 0.019*\"attack\" + 0.019*\"polic\" + 0.016*\"protest\" + 0.014*\"offic\" + 0.013*\"assault\" + 0.013*\"chines\" + 0.011*\"bodi\"\n",
      "Topic: 2 \n",
      " Words: 0.059*\"australia\" + 0.048*\"australian\" + 0.027*\"world\" + 0.018*\"test\" + 0.014*\"win\" + 0.011*\"farm\" + 0.011*\"final\" + 0.011*\"return\" + 0.010*\"beat\" + 0.009*\"cricket\"\n",
      "Topic: 3 \n",
      " Words: 0.031*\"polic\" + 0.030*\"charg\" + 0.027*\"court\" + 0.025*\"death\" + 0.025*\"murder\" + 0.021*\"woman\" + 0.018*\"die\" + 0.018*\"face\" + 0.017*\"alleg\" + 0.016*\"crash\"\n",
      "Topic: 4 \n",
      " Words: 0.018*\"chang\" + 0.014*\"speak\" + 0.014*\"power\" + 0.013*\"worker\" + 0.012*\"climat\" + 0.011*\"concern\" + 0.011*\"minist\" + 0.011*\"say\" + 0.011*\"john\" + 0.010*\"flood\"\n",
      "Topic: 5 \n",
      " Words: 0.021*\"market\" + 0.020*\"news\" + 0.018*\"women\" + 0.018*\"live\" + 0.016*\"tasmania\" + 0.013*\"high\" + 0.013*\"rise\" + 0.012*\"open\" + 0.012*\"price\" + 0.012*\"lose\"\n",
      "Topic: 6 \n",
      " Words: 0.036*\"elect\" + 0.018*\"water\" + 0.018*\"state\" + 0.016*\"tasmanian\" + 0.012*\"labor\" + 0.011*\"liber\" + 0.011*\"morrison\" + 0.011*\"parti\" + 0.010*\"campaign\" + 0.010*\"give\"\n",
      "Topic: 7 \n",
      " Words: 0.015*\"nation\" + 0.014*\"farmer\" + 0.013*\"time\" + 0.013*\"rural\" + 0.013*\"council\" + 0.013*\"indigen\" + 0.011*\"commiss\" + 0.011*\"plan\" + 0.011*\"drum\" + 0.011*\"communiti\"\n",
      "Topic: 8 \n",
      " Words: 0.043*\"trump\" + 0.035*\"year\" + 0.034*\"sydney\" + 0.027*\"queensland\" + 0.021*\"donald\" + 0.020*\"adelaid\" + 0.018*\"perth\" + 0.015*\"brisban\" + 0.015*\"peopl\" + 0.014*\"royal\"\n",
      "Topic: 9 \n",
      " Words: 0.030*\"govern\" + 0.020*\"warn\" + 0.017*\"say\" + 0.016*\"feder\" + 0.014*\"countri\" + 0.014*\"fund\" + 0.014*\"claim\" + 0.014*\"life\" + 0.012*\"health\" + 0.012*\"stori\"\n"
     ]
    }
   ],
   "source": [
    "for ind, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\n Words: {}'.format(ind, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index=0, topic=0.022*\"hous\" + 0.021*\"south\" + 0.019*\"canberra\" + 0.019*\"north\" + 0.016*\"bushfir\" + 0.016*\"miss\" + 0.013*\"interview\" + 0.011*\"hospit\" + 0.010*\"investig\" + 0.010*\"search\"\n",
      "\n",
      "\n",
      "\n",
      "index=1, topic=0.031*\"kill\" + 0.023*\"shoot\" + 0.021*\"dead\" + 0.019*\"attack\" + 0.019*\"polic\" + 0.016*\"protest\" + 0.014*\"offic\" + 0.013*\"assault\" + 0.013*\"chines\" + 0.011*\"bodi\"\n",
      "\n",
      "\n",
      "\n",
      "index=2, topic=0.059*\"australia\" + 0.048*\"australian\" + 0.027*\"world\" + 0.018*\"test\" + 0.014*\"win\" + 0.011*\"farm\" + 0.011*\"final\" + 0.011*\"return\" + 0.010*\"beat\" + 0.009*\"cricket\"\n",
      "\n",
      "\n",
      "\n",
      "index=3, topic=0.031*\"polic\" + 0.030*\"charg\" + 0.027*\"court\" + 0.025*\"death\" + 0.025*\"murder\" + 0.021*\"woman\" + 0.018*\"die\" + 0.018*\"face\" + 0.017*\"alleg\" + 0.016*\"crash\"\n",
      "\n",
      "\n",
      "\n",
      "index=4, topic=0.018*\"chang\" + 0.014*\"speak\" + 0.014*\"power\" + 0.013*\"worker\" + 0.012*\"climat\" + 0.011*\"concern\" + 0.011*\"minist\" + 0.011*\"say\" + 0.011*\"john\" + 0.010*\"flood\"\n",
      "\n",
      "\n",
      "\n",
      "index=5, topic=0.021*\"market\" + 0.020*\"news\" + 0.018*\"women\" + 0.018*\"live\" + 0.016*\"tasmania\" + 0.013*\"high\" + 0.013*\"rise\" + 0.012*\"open\" + 0.012*\"price\" + 0.012*\"lose\"\n",
      "\n",
      "\n",
      "\n",
      "index=6, topic=0.036*\"elect\" + 0.018*\"water\" + 0.018*\"state\" + 0.016*\"tasmanian\" + 0.012*\"labor\" + 0.011*\"liber\" + 0.011*\"morrison\" + 0.011*\"parti\" + 0.010*\"campaign\" + 0.010*\"give\"\n",
      "\n",
      "\n",
      "\n",
      "index=7, topic=0.015*\"nation\" + 0.014*\"farmer\" + 0.013*\"time\" + 0.013*\"rural\" + 0.013*\"council\" + 0.013*\"indigen\" + 0.011*\"commiss\" + 0.011*\"plan\" + 0.011*\"drum\" + 0.011*\"communiti\"\n",
      "\n",
      "\n",
      "\n",
      "index=8, topic=0.043*\"trump\" + 0.035*\"year\" + 0.034*\"sydney\" + 0.027*\"queensland\" + 0.021*\"donald\" + 0.020*\"adelaid\" + 0.018*\"perth\" + 0.015*\"brisban\" + 0.015*\"peopl\" + 0.014*\"royal\"\n",
      "\n",
      "\n",
      "\n",
      "index=9, topic=0.030*\"govern\" + 0.020*\"warn\" + 0.017*\"say\" + 0.016*\"feder\" + 0.014*\"countri\" + 0.014*\"fund\" + 0.014*\"claim\" + 0.014*\"life\" + 0.012*\"health\" + 0.012*\"stori\"\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, j in lda_model.show_topics():\n",
    "    print('index={}, topic={}'.format(i, j))\n",
    "    print(\"\\n\\n\")\n",
    "# print(lda_model.get_topic_terms())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the first topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************topn=10 start******************************\n",
      "word=hous, prob=0.02160000056028366\n",
      "\n",
      "word=south, prob=0.021490000188350677\n",
      "\n",
      "word=canberra, prob=0.01899000070989132\n",
      "\n",
      "word=north, prob=0.018859999254345894\n",
      "\n",
      "word=bushfir, prob=0.016330000013113022\n",
      "\n",
      "word=miss, prob=0.01565999910235405\n",
      "\n",
      "word=interview, prob=0.012900000438094139\n",
      "\n",
      "word=hospit, prob=0.010769999586045742\n",
      "\n",
      "word=investig, prob=0.010110000148415565\n",
      "\n",
      "word=search, prob=0.009759999811649323\n",
      "\n",
      "******************************topn=10 done******************************\n",
      "******************************topn=20 start******************************\n",
      "word=hous, prob=0.02160000056028366\n",
      "\n",
      "word=south, prob=0.021490000188350677\n",
      "\n",
      "word=canberra, prob=0.01899000070989132\n",
      "\n",
      "word=north, prob=0.018859999254345894\n",
      "\n",
      "word=bushfir, prob=0.016330000013113022\n",
      "\n",
      "word=miss, prob=0.01565999910235405\n",
      "\n",
      "word=interview, prob=0.012900000438094139\n",
      "\n",
      "word=hospit, prob=0.010769999586045742\n",
      "\n",
      "word=investig, prob=0.010110000148415565\n",
      "\n",
      "word=search, prob=0.009759999811649323\n",
      "\n",
      "word=hobart, prob=0.009580000303685665\n",
      "\n",
      "word=find, prob=0.00930000003427267\n",
      "\n",
      "word=rescu, prob=0.009189999662339687\n",
      "\n",
      "word=west, prob=0.008630000054836273\n",
      "\n",
      "word=continu, prob=0.00848000030964613\n",
      "\n",
      "word=korea, prob=0.008460000157356262\n",
      "\n",
      "word=white, prob=0.008340000174939632\n",
      "\n",
      "word=island, prob=0.008249999955296516\n",
      "\n",
      "word=second, prob=0.008030000142753124\n",
      "\n",
      "word=david, prob=0.007809999864548445\n",
      "\n",
      "******************************topn=20 done******************************\n"
     ]
    }
   ],
   "source": [
    "print(''.join([\"*\"*30])+\"topn=10 start\"+''.join([\"*\"*30]))\n",
    "for i, j in lda_model.show_topic(0, 10):\n",
    "    print('word={}, prob={}'.format(i, round(j, 5)))\n",
    "    print()\n",
    "print(''.join([\"*\"*30])+\"topn=10 done\"+''.join([\"*\"*30]))\n",
    "\n",
    "print(''.join([\"*\"*30])+\"topn=20 start\"+''.join([\"*\"*30]))\n",
    "for i, j in lda_model.show_topic(0, 20):\n",
    "    print('word={}, prob={}'.format(i, round(j, 5)))\n",
    "    print()\n",
    "print(''.join([\"*\"*30])+\"topn=20 done\"+''.join([\"*\"*30]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the 2nd argument in show_topic() function, viz **topn**, can be given values > 10(num_topics argument given during the model initialisation), and the topics are descending sorted w.r.t. their dirichlet distribution scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running LDA using the TFIDF corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with k=10 was instanced on 2020-06-01 18:24:41.035980\n",
      "time to complete model with k=10 was 0:03:44.551775\n",
      "Model with k=20 was instanced on 2020-06-01 18:28:55.443953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-51:\n",
      "Process ForkPoolWorker-52:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/laferrari/anaconda3/envs/tensorflow_cpu/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-e650959ec40f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtic1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model with k={} was instanced on {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtic1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mlda_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLdaMulticore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mtot_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtic1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"time to complete model with k={} was {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtot_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/gensim/models/ldamulticore.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, workers, chunksize, passes, batch, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, random_state, minimum_probability, minimum_phi_value, per_word_topics, dtype)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_every\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mgamma_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma_threshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminimum_probability\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminimum_probability\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0mminimum_phi_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminimum_phi_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_word_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mper_word_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         )\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0muse_numpy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks_as_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_numpy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit_dir_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/gensim/models/ldamulticore.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, corpus, chunks_as_numpy)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0mchunk_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunks_as_numpy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mchunk_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m                 \u001b[0mreallen\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# keep track of how many documents we've processed so far\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mchunkize_serial\u001b[0;34m(iterable, chunksize, as_numpy, dtype)\u001b[0m\n\u001b[1;32m   1168\u001b[0m             \u001b[0mwrapped_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1170\u001b[0;31m             \u001b[0mwrapped_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwrapped_chunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/gensim/interfaces.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocno\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/gensim/models/tfidfmodel.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, bow, eps)\u001b[0m\n\u001b[1;32m    507\u001b[0m         vector = [\n\u001b[1;32m    508\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mtermid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtermid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mtermid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtermid_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_array\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtermid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m         ]\n\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/gensim/models/tfidfmodel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    507\u001b[0m         vector = [\n\u001b[1;32m    508\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mtermid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtermid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mtermid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtermid_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_array\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtermid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m         ]\n\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/laferrari/anaconda3/envs/tensorflow_cpu/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/laferrari/anaconda3/envs/tensorflow_cpu/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/laferrari/anaconda3/envs/tensorflow_cpu/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/laferrari/anaconda3/envs/tensorflow_cpu/lib/python3.6/multiprocessing/pool.py\", line 103, in worker\n",
      "    initializer(*initargs)\n",
      "  File \"/home/laferrari/anaconda3/envs/tensorflow_cpu/lib/python3.6/multiprocessing/pool.py\", line 103, in worker\n",
      "    initializer(*initargs)\n",
      "  File \"/home/laferrari/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/gensim/models/ldamulticore.py\", line 337, in worker_e_step\n",
      "    worker_lda.do_estep(chunk)  # TODO: auto-tune alpha?\n",
      "  File \"/home/laferrari/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/gensim/models/ldamulticore.py\", line 337, in worker_e_step\n",
      "    worker_lda.do_estep(chunk)  # TODO: auto-tune alpha?\n",
      "  File \"/home/laferrari/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/gensim/models/ldamodel.py\", line 742, in do_estep\n",
      "    gamma, sstats = self.inference(chunk, collect_sstats=True)\n",
      "  File \"/home/laferrari/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/gensim/models/ldamodel.py\", line 742, in do_estep\n",
      "    gamma, sstats = self.inference(chunk, collect_sstats=True)\n",
      "  File \"/home/laferrari/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/gensim/models/ldamodel.py\", line 694, in inference\n",
      "    Elogthetad = dirichlet_expectation(gammad)\n",
      "  File \"/home/laferrari/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/gensim/models/ldamodel.py\", line 696, in inference\n",
      "    phinorm = np.dot(expElogthetad, expElogbetad) + epsilon\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaMulticore, CoherenceModel\n",
    "time_list, coherence_scores = [], {}\n",
    "for k in range(10, 100, 10):\n",
    "    tic1 = datetime.now()\n",
    "    print(\"Model with k={} was instanced on {}\".format(k, tic1))\n",
    "    lda_model = LdaMulticore(corpus_tfidf, num_topics=k, id2word=dictionary, passes=1, workers=3)\n",
    "    tot_time = datetime.now()-tic1\n",
    "    print(\"time to complete model with k={} was {}\".format(k, tot_time))\n",
    "    time_list.append(tot_time)\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_docs, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    coherence_scores[k] = coherence_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(list(coherence_scores.keys()), list(coherence_scores.values()), c='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training time for model is 0:05:06.376622\n",
      "Topic: 0 \n",
      "Words: 0.017*\"crash\" + 0.010*\"die\" + 0.010*\"climat\" + 0.008*\"miss\" + 0.008*\"wednesday\" + 0.008*\"fatal\" + 0.008*\"sport\" + 0.008*\"search\" + 0.007*\"grandstand\" + 0.007*\"peter\"\n",
      "Topic: 1 \n",
      "Words: 0.022*\"news\" + 0.016*\"rural\" + 0.012*\"queensland\" + 0.008*\"friday\" + 0.008*\"nation\" + 0.007*\"drought\" + 0.007*\"farm\" + 0.007*\"farmer\" + 0.007*\"morrison\" + 0.007*\"dollar\"\n",
      "Topic: 2 \n",
      "Words: 0.028*\"trump\" + 0.012*\"royal\" + 0.009*\"commiss\" + 0.009*\"monday\" + 0.008*\"thursday\" + 0.007*\"john\" + 0.006*\"senat\" + 0.006*\"say\" + 0.006*\"abbott\" + 0.006*\"tree\"\n",
      "Topic: 3 \n",
      "Words: 0.012*\"interview\" + 0.010*\"market\" + 0.010*\"australia\" + 0.008*\"australian\" + 0.008*\"live\" + 0.007*\"world\" + 0.007*\"cricket\" + 0.006*\"david\" + 0.006*\"share\" + 0.006*\"zealand\"\n",
      "Topic: 4 \n",
      "Words: 0.010*\"scott\" + 0.008*\"leagu\" + 0.008*\"rugbi\" + 0.007*\"outback\" + 0.007*\"histori\" + 0.007*\"jam\" + 0.006*\"brief\" + 0.005*\"grand\" + 0.005*\"georg\" + 0.005*\"data\"\n",
      "Topic: 5 \n",
      "Words: 0.012*\"govern\" + 0.012*\"countri\" + 0.010*\"hour\" + 0.007*\"fund\" + 0.006*\"plan\" + 0.006*\"turnbul\" + 0.005*\"feder\" + 0.005*\"say\" + 0.005*\"health\" + 0.005*\"chang\"\n",
      "Topic: 6 \n",
      "Words: 0.014*\"murder\" + 0.010*\"polic\" + 0.010*\"death\" + 0.009*\"court\" + 0.008*\"sentenc\" + 0.008*\"jail\" + 0.007*\"charg\" + 0.007*\"prison\" + 0.006*\"juli\" + 0.006*\"trial\"\n",
      "Topic: 7 \n",
      "Words: 0.006*\"novemb\" + 0.006*\"energi\" + 0.006*\"young\" + 0.006*\"foreign\" + 0.005*\"hobart\" + 0.005*\"capit\" + 0.005*\"destroy\" + 0.005*\"candid\" + 0.005*\"hill\" + 0.005*\"product\"\n",
      "Topic: 8 \n",
      "Words: 0.015*\"donald\" + 0.014*\"drum\" + 0.013*\"charg\" + 0.012*\"stori\" + 0.009*\"alleg\" + 0.009*\"tuesday\" + 0.009*\"guilti\" + 0.008*\"michael\" + 0.008*\"assault\" + 0.008*\"polic\"\n",
      "Topic: 9 \n",
      "Words: 0.013*\"south\" + 0.010*\"weather\" + 0.010*\"north\" + 0.010*\"christma\" + 0.009*\"kill\" + 0.008*\"korea\" + 0.007*\"west\" + 0.006*\"june\" + 0.006*\"polic\" + 0.006*\"dead\"\n"
     ]
    }
   ],
   "source": [
    "tic1 = datetime.now()\n",
    "lda_tfidf_model = LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
    "tot_time = datetime.now()-tic1\n",
    "print('total training time for model is', tot_time)\n",
    "for ind, topic in lda_tfidf_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(ind, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance evaluation by classifying sample document using LDA Bag of Words model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize topic keywords\n",
    "\n",
    "* Each bubble on the left-hand side plot represents a topic. The larger the bubble, the more prevalent is that topic.\n",
    "\n",
    "* A good topic model will have fairly big, non-overlapping bubbles scattered throughout the chart instead of being clustered in one quadrant.\n",
    "\n",
    "* A model with too many topics, will typically have many overlaps, small sized bubbles clustered in one region of the chart.\n",
    "\n",
    "* Alright, if you move the cursor over one of the bubbles, the words and bars on the right-hand side will update. These words are the salient keywords that form the selected topic.\n",
    "\n",
    "On surfing the net, I found that the default algorithm, *mds*(pcoa), is <font color=\"red\">unstable</font>, and hence the *prepare* step takes about 17 min.\\\n",
    "Hence use the mmds algorithm instead.\\\n",
    "<font color=\"red\" size=\"4\">Note: </font>If this still doesn't improve the speed, it might be because of running in a virtual environment. [Comments of this post](https://stackoverflow.com/questions/56707416/pyldavis-prepare-is-slow) claim that running inside a virtual environment results in strikingly slower speed, w.r.t. python3 installed on machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to enable pyLDAvis is:0:00:00.000205\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laferrari/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to prepare pyLDAvis is:0:17:44.587514\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el69891403026246480728488048203\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el69891403026246480728488048203_data = {\"mdsDat\": {\"x\": [-0.18745070161036118, 0.20371383424757541, 0.052966744630255955, -0.1703575301777974, -0.208822476583963, 0.09500601091818041, 0.055003016157623, -0.2046567686233254, 0.16349029063270695, 0.2011075804091054], \"y\": [0.08418400950113879, 0.1896954248371766, -0.2871775153436378, 0.026990792250536026, 0.08044300744911276, -0.28274691465214746, -0.05099947485647483, -0.004956398294462122, 0.18041837780913056, 0.06414869129962711], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [11.980048179626465, 10.884151458740234, 10.4362211227417, 10.327317237854004, 10.089188575744629, 10.015703201293945, 9.662029266357422, 9.233928680419922, 8.768363952636719, 8.603041648864746]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"Freq\": [31594.0, 25612.0, 19623.0, 17587.0, 30244.0, 16324.0, 17452.0, 15605.0, 16313.0, 14739.0, 15654.0, 14586.0, 14710.0, 14490.0, 12282.0, 12069.0, 11760.0, 10731.0, 11158.0, 11102.0, 10993.0, 13767.0, 10523.0, 9809.0, 9786.0, 10364.0, 9810.0, 13486.0, 10111.0, 10264.0, 9274.6357421875, 8644.689453125, 8613.724609375, 8227.814453125, 8240.0126953125, 7128.740234375, 6881.38818359375, 6795.587890625, 6547.2490234375, 6283.09375, 5600.48681640625, 5467.40234375, 4827.4228515625, 4508.32080078125, 4268.05810546875, 4151.7822265625, 4004.618408203125, 3847.810302734375, 3841.10986328125, 3710.764404296875, 3540.19287109375, 3448.387939453125, 3349.286376953125, 3039.63427734375, 3042.28173828125, 3576.885009765625, 2636.54150390625, 2620.4326171875, 2598.448486328125, 2570.546142578125, 6219.7060546875, 4840.42626953125, 3581.1904296875, 3419.678955078125, 4558.20068359375, 9335.47265625, 6884.81494140625, 6614.375, 6573.94775390625, 4311.2255859375, 4448.66259765625, 4020.524169921875, 4518.00048828125, 15653.9228515625, 14709.4375, 14489.734375, 10363.7802734375, 10263.341796875, 9988.1630859375, 7699.09130859375, 6954.96826171875, 6730.458984375, 6689.86083984375, 6165.6611328125, 17449.763671875, 6092.81591796875, 5318.9755859375, 4280.4609375, 3957.336181640625, 3950.22412109375, 3867.5361328125, 3812.48876953125, 3720.919677734375, 3684.975830078125, 3663.527099609375, 5593.3115234375, 3326.244140625, 3305.261962890625, 3275.221435546875, 3082.28369140625, 3076.32861328125, 2959.22216796875, 2946.059814453125, 12065.9072265625, 8063.02587890625, 9110.0458984375, 17856.83203125, 4542.8798828125, 4859.51708984375, 4479.93017578125, 4652.61181640625, 3961.435546875, 3857.329345703125, 11759.7587890625, 10110.3876953125, 9777.724609375, 10992.1962890625, 8814.744140625, 7081.32421875, 7081.94091796875, 6778.8828125, 6675.99365234375, 6446.9677734375, 6420.77001953125, 6017.2919921875, 6185.8681640625, 5269.83984375, 4955.19384765625, 4903.13623046875, 4795.19140625, 4641.3466796875, 4481.7939453125, 4194.94482421875, 3730.66650390625, 3493.353759765625, 3418.412353515625, 3406.36474609375, 3384.207275390625, 3357.3427734375, 3305.8515625, 3179.23046875, 3137.93701171875, 3127.05615234375, 5350.287109375, 5978.5263671875, 6839.6806640625, 5316.96826171875, 5465.2158203125, 3522.554931640625, 3837.231201171875, 7852.0830078125, 7783.666015625, 6937.01513671875, 6433.20654296875, 6184.09716796875, 5817.10205078125, 5499.53662109375, 5406.400390625, 5352.97412109375, 5788.70556640625, 4905.21630859375, 4729.8623046875, 4669.6669921875, 4390.474609375, 4205.03076171875, 3963.825927734375, 3958.0537109375, 3880.504638671875, 3851.767578125, 3795.78515625, 3708.3291015625, 3699.277099609375, 3667.325927734375, 3634.27392578125, 3461.21875, 3451.151611328125, 3356.983642578125, 3354.306884765625, 3238.433837890625, 3215.810791015625, 10033.9306640625, 6001.43212890625, 5027.08642578125, 3841.86572265625, 4805.47607421875, 3993.380859375, 5939.55517578125, 3842.37744140625, 16312.94140625, 10522.6552734375, 7781.82958984375, 7762.5341796875, 7574.837890625, 7499.50927734375, 6243.25146484375, 5839.07373046875, 5701.81201171875, 5697.0595703125, 5449.1875, 5581.3583984375, 4938.91796875, 4685.72607421875, 4429.71435546875, 4207.44775390625, 4057.583251953125, 4025.431640625, 3966.421875, 3768.161865234375, 3619.54052734375, 3310.981689453125, 2877.772705078125, 2832.239990234375, 2818.931640625, 2788.004150390625, 2755.029541015625, 2513.0712890625, 2479.11279296875, 2469.07177734375, 8455.396484375, 4652.498046875, 6471.470703125, 9351.189453125, 3049.86181640625, 31593.607421875, 25611.734375, 14585.8291015625, 9426.458984375, 7273.90625, 6059.91064453125, 5812.16796875, 5258.87158203125, 4744.388671875, 3859.181884765625, 3771.798583984375, 3620.17626953125, 3596.684814453125, 3540.117431640625, 3149.56982421875, 3143.370849609375, 3067.29296875, 2978.635498046875, 2905.865234375, 2822.66259765625, 2813.969482421875, 2747.516357421875, 2723.1435546875, 2688.18359375, 2678.47509765625, 2661.06640625, 2639.12109375, 2627.786865234375, 2536.70654296875, 2415.206298828125, 5907.77734375, 3650.015625, 3643.826416015625, 3265.919189453125, 3223.113525390625, 2812.29296875, 11158.1318359375, 11101.29296875, 9810.107421875, 8435.1259765625, 8088.5078125, 6664.7578125, 5039.17529296875, 4950.6953125, 4801.68994140625, 4748.07958984375, 4381.72265625, 4307.50390625, 4372.07666015625, 4148.93212890625, 4034.089599609375, 3867.85693359375, 3540.890869140625, 3496.93115234375, 3324.93603515625, 3319.520751953125, 3294.809326171875, 3187.974853515625, 3175.81982421875, 3088.591552734375, 3076.41943359375, 2968.598388671875, 2952.9013671875, 2827.036865234375, 2692.150634765625, 2683.0908203125, 9739.7373046875, 5223.85888671875, 5564.42724609375, 4455.5029296875, 4262.552734375, 3643.7216796875, 17586.23828125, 9081.08984375, 8795.03515625, 7774.90869140625, 5660.4794921875, 5511.1748046875, 5268.16552734375, 4925.9326171875, 4781.1552734375, 4669.26318359375, 4451.5419921875, 4382.99560546875, 3976.09033203125, 3873.888671875, 3597.6279296875, 3528.755126953125, 3318.46533203125, 3286.296630859375, 3103.881591796875, 2986.40478515625, 2902.551025390625, 2753.52978515625, 2742.26025390625, 2604.576904296875, 2599.475341796875, 2433.497802734375, 2413.071044921875, 2359.791259765625, 2355.258544921875, 2294.630126953125, 3270.905517578125, 6070.16357421875, 3876.282958984375, 3850.6474609375, 3286.9208984375, 3672.877685546875, 3476.242919921875, 3311.697265625, 14738.587890625, 10730.3271484375, 9785.4775390625, 6363.87841796875, 6190.19287109375, 5932.15673828125, 5317.77880859375, 5218.74609375, 5100.7822265625, 5007.4013671875, 4986.90234375, 4825.958984375, 3857.01318359375, 3779.648193359375, 3688.26513671875, 3382.6005859375, 3109.41015625, 3068.5625, 2794.598388671875, 2643.58837890625, 2604.318603515625, 2583.695556640625, 2503.328125, 2474.2421875, 2451.379638671875, 2424.79736328125, 2268.0771484375, 2267.79150390625, 2261.119384765625, 2203.18994140625, 4133.7041015625, 7627.73779296875, 9114.578125, 5062.8525390625, 4186.26904296875, 4555.7568359375, 9031.7666015625, 2909.37646484375, 2912.2021484375, 3509.13525390625, 19622.970703125, 16324.0224609375, 15604.701171875, 12281.9169921875, 9808.8583984375, 9236.5224609375, 8168.8525390625, 7006.0712890625, 6741.068359375, 6626.63037109375, 5721.1962890625, 5536.2939453125, 5035.13037109375, 4366.60107421875, 3784.91357421875, 3715.30517578125, 3321.600830078125, 3053.141357421875, 3030.89990234375, 3013.22314453125, 2996.18212890625, 2950.76953125, 2721.3125, 2458.614013671875, 2433.775146484375, 2342.90966796875, 2282.216552734375, 2162.3388671875, 2038.40478515625, 1997.1025390625, 4145.02294921875, 3458.74072265625, 4989.03662109375, 3804.71533203125, 3761.11962890625, 4142.7890625, 2920.907958984375, 3355.546142578125, 3078.2958984375], \"Term\": [\"australia\", \"australian\", \"trump\", \"elect\", \"polic\", \"year\", \"charg\", \"sydney\", \"govern\", \"kill\", \"court\", \"world\", \"death\", \"murder\", \"queensland\", \"woman\", \"market\", \"shoot\", \"hous\", \"south\", \"news\", \"attack\", \"warn\", \"donald\", \"dead\", \"die\", \"canberra\", \"chang\", \"women\", \"face\", \"farmer\", \"time\", \"rural\", \"indigen\", \"council\", \"commiss\", \"drum\", \"communiti\", \"industri\", \"park\", \"centr\", \"care\", \"food\", \"research\", \"age\", \"tuesday\", \"western\", \"debat\", \"parent\", \"sport\", \"grow\", \"develop\", \"tree\", \"mayor\", \"princ\", \"protect\", \"remot\", \"dairi\", \"tourism\", \"nurs\", \"drought\", \"aborigin\", \"youth\", \"outback\", \"group\", \"nation\", \"plan\", \"school\", \"help\", \"region\", \"take\", \"student\", \"say\", \"court\", \"death\", \"murder\", \"die\", \"face\", \"alleg\", \"trial\", \"case\", \"guilti\", \"victoria\", \"road\", \"charg\", \"driver\", \"victorian\", \"teen\", \"fatal\", \"stab\", \"drive\", \"plead\", \"hong\", \"kong\", \"fire\", \"hear\", \"steal\", \"truck\", \"threat\", \"lawyer\", \"brief\", \"teenag\", \"father\", \"woman\", \"accus\", \"crash\", \"polic\", \"tell\", \"arrest\", \"jail\", \"attack\", \"child\", \"releas\", \"market\", \"women\", \"live\", \"news\", \"tasmania\", \"rise\", \"high\", \"price\", \"lose\", \"break\", \"street\", \"gold\", \"fall\", \"scott\", \"wall\", \"share\", \"turnbul\", \"leagu\", \"week\", \"hit\", \"dollar\", \"player\", \"wednesday\", \"cyclon\", \"rugbi\", \"hill\", \"sale\", \"jam\", \"club\", \"properti\", \"million\", \"record\", \"open\", \"busi\", \"coast\", \"look\", \"china\", \"speak\", \"power\", \"worker\", \"climat\", \"concern\", \"john\", \"fear\", \"emerg\", \"christma\", \"flood\", \"station\", \"hour\", \"weather\", \"compani\", \"financ\", \"energi\", \"fish\", \"stop\", \"fail\", \"delay\", \"retir\", \"job\", \"threaten\", \"kid\", \"murray\", \"union\", \"footag\", \"thousand\", \"mine\", \"coal\", \"chang\", \"minist\", \"meet\", \"action\", \"servic\", \"storm\", \"say\", \"deal\", \"govern\", \"warn\", \"countri\", \"fund\", \"claim\", \"life\", \"stori\", \"polit\", \"make\", \"presid\", \"reveal\", \"budget\", \"media\", \"senat\", \"issu\", \"mental\", \"peter\", \"appeal\", \"local\", \"deni\", \"histori\", \"alan\", \"hand\", \"reject\", \"islam\", \"fiji\", \"slam\", \"data\", \"convict\", \"paul\", \"feder\", \"public\", \"health\", \"say\", \"call\", \"australia\", \"australian\", \"world\", \"test\", \"win\", \"farm\", \"return\", \"beat\", \"cricket\", \"play\", \"right\", \"monday\", \"india\", \"team\", \"project\", \"explain\", \"tour\", \"coach\", \"grand\", \"best\", \"juli\", \"disabl\", \"england\", \"smith\", \"territori\", \"human\", \"olymp\", \"music\", \"johnson\", \"franc\", \"final\", \"race\", \"turn\", \"lead\", \"say\", \"open\", \"hous\", \"south\", \"canberra\", \"bushfir\", \"miss\", \"interview\", \"search\", \"hobart\", \"find\", \"rescu\", \"continu\", \"white\", \"korea\", \"second\", \"david\", \"girl\", \"andrew\", \"east\", \"thursday\", \"inquest\", \"pacif\", \"black\", \"challeng\", \"plane\", \"film\", \"social\", \"result\", \"indonesia\", \"begin\", \"william\", \"north\", \"investig\", \"hospit\", \"west\", \"island\", \"coast\", \"elect\", \"water\", \"state\", \"tasmanian\", \"liber\", \"morrison\", \"parti\", \"campaign\", \"give\", \"green\", \"season\", \"futur\", \"friday\", \"zealand\", \"light\", \"cancer\", \"polici\", \"crime\", \"facebook\", \"money\", \"killer\", \"long\", \"ban\", \"check\", \"shorten\", \"manag\", \"control\", \"femal\", \"candid\", \"love\", \"anti\", \"labor\", \"save\", \"leader\", \"go\", \"need\", \"vote\", \"nation\", \"kill\", \"shoot\", \"dead\", \"offic\", \"assault\", \"chines\", \"bodi\", \"michael\", \"violenc\", \"parliament\", \"program\", \"sexual\", \"refuge\", \"amid\", \"great\", \"bomb\", \"domest\", \"number\", \"suicid\", \"comment\", \"militari\", \"expect\", \"british\", \"rock\", \"offici\", \"hunt\", \"apologis\", \"oper\", \"coupl\", \"june\", \"strike\", \"protest\", \"attack\", \"bank\", \"forc\", \"victim\", \"polic\", \"port\", \"town\", \"say\", \"trump\", \"year\", \"sydney\", \"queensland\", \"donald\", \"adelaid\", \"perth\", \"brisban\", \"peopl\", \"royal\", \"drug\", \"abus\", \"darwin\", \"babi\", \"hold\", \"head\", \"mark\", \"celebr\", \"insid\", \"rat\", \"burn\", \"russia\", \"extend\", \"catch\", \"know\", \"super\", \"america\", \"rape\", \"onlin\", \"church\", \"young\", \"beach\", \"sentenc\", \"prison\", \"game\", \"jail\", \"video\", \"polic\", \"home\"], \"Total\": [31594.0, 25612.0, 19623.0, 17587.0, 30244.0, 16324.0, 17452.0, 15605.0, 16313.0, 14739.0, 15654.0, 14586.0, 14710.0, 14490.0, 12282.0, 12069.0, 11760.0, 10731.0, 11158.0, 11102.0, 10993.0, 13767.0, 10523.0, 9809.0, 9786.0, 10364.0, 9810.0, 13486.0, 10111.0, 10264.0, 9275.4482421875, 8645.501953125, 8614.537109375, 8228.6298828125, 8240.8388671875, 7129.552734375, 6882.20068359375, 6796.400390625, 6548.0615234375, 6283.90625, 5601.29931640625, 5468.21484375, 4828.2353515625, 4509.13330078125, 4268.87060546875, 4152.5947265625, 4005.43115234375, 3848.622802734375, 3841.922607421875, 3711.5771484375, 3541.00537109375, 3449.20068359375, 3350.098876953125, 3040.44677734375, 3043.094482421875, 3577.923828125, 2637.35400390625, 2621.2451171875, 2599.260986328125, 2571.358642578125, 6224.21435546875, 4850.68701171875, 3585.138427734375, 3431.931396484375, 4833.765625, 12647.8916015625, 11670.9140625, 11457.990234375, 11533.9736328125, 6081.96484375, 7289.19482421875, 6811.654296875, 29317.458984375, 15654.74609375, 14710.2607421875, 14490.5576171875, 10364.603515625, 10264.1650390625, 9988.986328125, 7699.91357421875, 6955.79052734375, 6731.28125, 6690.68310546875, 6166.484375, 17452.107421875, 6093.63818359375, 5319.7978515625, 4281.283203125, 3958.158935546875, 3951.046875, 3868.35888671875, 3813.3115234375, 3721.742431640625, 3685.798583984375, 3664.349853515625, 5594.6171875, 3327.06689453125, 3306.084716796875, 3276.044189453125, 3083.1064453125, 3077.151611328125, 2960.044921875, 2946.882568359375, 12069.939453125, 9049.8388671875, 11813.3798828125, 30244.79296875, 6122.41259765625, 7606.26025390625, 8623.4658203125, 13767.9140625, 7122.4951171875, 6118.1748046875, 11760.5859375, 10111.21484375, 9778.5517578125, 10993.15234375, 8815.5712890625, 7082.1513671875, 7082.76806640625, 6779.7099609375, 6676.82080078125, 6447.794921875, 6421.59716796875, 6018.119140625, 6186.74951171875, 5270.6669921875, 4956.02099609375, 4903.96337890625, 4796.0185546875, 4642.173828125, 4482.62158203125, 4195.77197265625, 3731.49365234375, 3494.18115234375, 3419.239501953125, 3407.19189453125, 3385.03466796875, 3358.169921875, 3306.6787109375, 3180.057861328125, 3138.76416015625, 3127.88330078125, 5496.11962890625, 7826.00537109375, 9652.7138671875, 7405.58251953125, 9109.677734375, 4758.5341796875, 11976.00390625, 7852.8974609375, 7784.48046875, 6937.82958984375, 6434.02099609375, 6184.91162109375, 5817.91650390625, 5500.35107421875, 5407.21484375, 5353.78857421875, 5789.59619140625, 4906.03076171875, 4730.6767578125, 4670.4814453125, 4391.2890625, 4205.84521484375, 3964.640625, 3958.868408203125, 3881.3193359375, 3852.582275390625, 3796.599853515625, 3709.14404296875, 3700.091796875, 3668.140625, 3635.088623046875, 3462.033447265625, 3451.96630859375, 3357.79833984375, 3355.123291015625, 3239.24853515625, 3216.62548828125, 13486.537109375, 7851.73681640625, 6394.24169921875, 4238.37548828125, 6633.36376953125, 5130.1435546875, 29317.458984375, 6661.4033203125, 16313.7587890625, 10523.4853515625, 7782.6484375, 7763.35205078125, 7575.65576171875, 7500.3271484375, 6244.0693359375, 5839.8916015625, 5702.6298828125, 5697.87744140625, 5450.00537109375, 5582.22998046875, 4939.73583984375, 4686.5439453125, 4430.5322265625, 4208.265625, 4058.4013671875, 4026.249755859375, 3967.239990234375, 3768.981201171875, 3620.35888671875, 3311.800048828125, 2878.5908203125, 2833.05810546875, 2819.749755859375, 2788.822265625, 2755.84765625, 2513.889404296875, 2479.930908203125, 2469.889892578125, 9691.8203125, 5472.39208984375, 9840.68359375, 29317.458984375, 10908.380859375, 31594.435546875, 25612.5625, 14586.6552734375, 9427.291015625, 7274.83447265625, 6060.73779296875, 5812.9951171875, 5259.69921875, 4745.2158203125, 3860.015625, 3772.6259765625, 3621.00341796875, 3597.511962890625, 3540.944580078125, 3150.39697265625, 3144.197998046875, 3068.1201171875, 2979.462646484375, 2906.6923828125, 2823.48974609375, 2814.796630859375, 2748.34375, 2723.970703125, 2689.0107421875, 2679.302490234375, 2661.89453125, 2639.9482421875, 2628.614013671875, 2537.53369140625, 2416.033447265625, 8339.6689453125, 4679.173828125, 4744.4228515625, 4253.9853515625, 29317.458984375, 9652.7138671875, 11158.958984375, 11102.1201171875, 9810.935546875, 8435.953125, 8089.3349609375, 6665.5849609375, 5040.00244140625, 4951.5224609375, 4802.51708984375, 4748.90673828125, 4382.55029296875, 4308.3310546875, 4372.923828125, 4149.75927734375, 4034.9169921875, 3868.684326171875, 3541.71826171875, 3497.758544921875, 3325.763427734375, 3320.34814453125, 3295.63671875, 3188.80224609375, 3176.647216796875, 3089.4189453125, 3077.246826171875, 2969.42578125, 2953.728759765625, 2827.8642578125, 2692.97802734375, 2683.918212890625, 9744.1611328125, 7245.71337890625, 8031.02880859375, 6168.14013671875, 8272.064453125, 9109.677734375, 17587.05859375, 9081.91015625, 8795.85546875, 7775.72900390625, 5661.2998046875, 5511.99560546875, 5268.98583984375, 4926.7548828125, 4781.9755859375, 4670.08349609375, 4452.3623046875, 4383.81591796875, 3976.9111328125, 3874.70947265625, 3598.44873046875, 3529.575927734375, 3319.2861328125, 3287.117431640625, 3104.702392578125, 2987.2255859375, 2903.371826171875, 2754.3505859375, 2743.0810546875, 2605.397705078125, 2600.296142578125, 2434.318603515625, 2413.891845703125, 2360.612060546875, 2356.079345703125, 2295.450927734375, 3289.083251953125, 8313.865234375, 5493.42578125, 5438.013671875, 4167.64990234375, 5952.50634765625, 5509.060546875, 12647.8916015625, 14739.4033203125, 10731.142578125, 9786.5078125, 6364.69384765625, 6191.00830078125, 5932.97216796875, 5318.59423828125, 5219.5615234375, 5101.59765625, 5008.216796875, 4987.7177734375, 4826.7744140625, 3857.828857421875, 3780.4638671875, 3689.080810546875, 3383.416259765625, 3110.225830078125, 3069.378173828125, 2795.4140625, 2644.404052734375, 2605.13427734375, 2584.51123046875, 2504.143798828125, 2475.05810546875, 2452.1953125, 2425.613037109375, 2268.892822265625, 2268.607177734375, 2261.93505859375, 2204.005615234375, 4143.45166015625, 9835.2626953125, 13767.9140625, 7564.43994140625, 5688.826171875, 7051.32666015625, 30244.79296875, 4350.830078125, 5003.16064453125, 29317.458984375, 19623.80859375, 16324.8603515625, 15605.5390625, 12282.7548828125, 9809.6962890625, 9237.3603515625, 8169.6904296875, 7006.9091796875, 6741.90625, 6627.46826171875, 5722.0341796875, 5537.1318359375, 5035.96826171875, 4367.43896484375, 3785.75146484375, 3716.142822265625, 3322.438720703125, 3053.97900390625, 3031.73779296875, 3014.06103515625, 2997.02001953125, 2951.607421875, 2722.150146484375, 2459.451904296875, 2434.613037109375, 2343.74755859375, 2283.054443359375, 2163.1767578125, 2039.2425537109375, 1997.9403076171875, 4165.63134765625, 3515.297607421875, 6295.40283203125, 5070.65576171875, 5644.47412109375, 8623.4658203125, 4424.09912109375, 30244.79296875, 11361.203125], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 2.121799945831299, 2.121799945831299, 2.121799945831299, 2.121799945831299, 2.121799945831299, 2.121799945831299, 2.121799945831299, 2.121799945831299, 2.121799945831299, 2.121799945831299, 2.121799945831299, 2.121799945831299, 2.121799945831299, 2.1217000484466553, 2.1217000484466553, 2.1217000484466553, 2.1217000484466553, 2.1217000484466553, 2.1217000484466553, 2.1217000484466553, 2.1217000484466553, 2.1217000484466553, 2.1217000484466553, 2.1217000484466553, 2.1217000484466553, 2.1215999126434326, 2.1215999126434326, 2.1215999126434326, 2.1215999126434326, 2.1215999126434326, 2.1212000846862793, 2.119800090789795, 2.120800018310547, 2.1184000968933105, 2.063199996948242, 1.8183000087738037, 1.594099998474121, 1.5724999904632568, 1.5597000122070312, 1.7777999639511108, 1.628100037574768, 1.5946999788284302, 0.251800000667572, 2.2177999019622803, 2.2177999019622803, 2.2177999019622803, 2.2177999019622803, 2.2177999019622803, 2.2177999019622803, 2.2177999019622803, 2.2177000045776367, 2.2177000045776367, 2.2177000045776367, 2.2177000045776367, 2.2177000045776367, 2.2177000045776367, 2.2177000045776367, 2.2177000045776367, 2.2177000045776367, 2.2177000045776367, 2.217600107192993, 2.217600107192993, 2.217600107192993, 2.217600107192993, 2.217600107192993, 2.217600107192993, 2.217600107192993, 2.217600107192993, 2.217600107192993, 2.217600107192993, 2.217600107192993, 2.217600107192993, 2.217600107192993, 2.2174999713897705, 2.102400064468384, 1.9579999446868896, 1.6908999681472778, 1.9194999933242798, 1.7697999477386475, 1.562999963760376, 1.1330000162124634, 1.6311999559402466, 1.756600022315979, 2.2597999572753906, 2.2597999572753906, 2.2597999572753906, 2.2597999572753906, 2.2597999572753906, 2.2597999572753906, 2.2597999572753906, 2.2597999572753906, 2.2597999572753906, 2.2597999572753906, 2.2597999572753906, 2.2597999572753906, 2.259700059890747, 2.259700059890747, 2.259700059890747, 2.259700059890747, 2.259700059890747, 2.259700059890747, 2.259700059890747, 2.259700059890747, 2.259700059890747, 2.259700059890747, 2.2595999240875244, 2.2595999240875244, 2.2595999240875244, 2.2595999240875244, 2.2595999240875244, 2.2595999240875244, 2.2595999240875244, 2.2595999240875244, 2.2330000400543213, 1.9905999898910522, 1.9154000282287598, 1.9285999536514282, 1.7489999532699585, 1.9591000080108643, 1.1217000484466553, 2.2702999114990234, 2.2702999114990234, 2.2702999114990234, 2.2702999114990234, 2.27020001411438, 2.27020001411438, 2.27020001411438, 2.27020001411438, 2.27020001411438, 2.27020001411438, 2.27020001411438, 2.27020001411438, 2.27020001411438, 2.27020001411438, 2.27020001411438, 2.27020001411438, 2.27020001411438, 2.27020001411438, 2.27020001411438, 2.27020001411438, 2.27020001411438, 2.27020001411438, 2.27020001411438, 2.27020001411438, 2.2701001167297363, 2.2701001167297363, 2.2701001167297363, 2.2701001167297363, 2.2701001167297363, 2.2701001167297363, 1.9746999740600586, 2.0016000270843506, 2.0297999382019043, 2.1721999645233154, 1.9479999542236328, 2.01990008354187, 0.673799991607666, 1.7201000452041626, 2.2936999797821045, 2.293600082397461, 2.293600082397461, 2.293600082397461, 2.293600082397461, 2.293600082397461, 2.293600082397461, 2.293600082397461, 2.293600082397461, 2.293600082397461, 2.293600082397461, 2.2934999465942383, 2.2934999465942383, 2.2934999465942383, 2.2934999465942383, 2.2934999465942383, 2.2934999465942383, 2.2934999465942383, 2.2934999465942383, 2.2934999465942383, 2.2934999465942383, 2.2934999465942383, 2.2934000492095947, 2.2934000492095947, 2.2934000492095947, 2.2934000492095947, 2.2934000492095947, 2.2934000492095947, 2.2934000492095947, 2.2934000492095947, 2.1572000980377197, 2.1314001083374023, 1.8746000528335571, 1.1510000228881836, 1.0192999839782715, 2.3010001182556152, 2.3010001182556152, 2.3010001182556152, 2.3008999824523926, 2.3008999824523926, 2.3008999824523926, 2.3008999824523926, 2.3008999824523926, 2.300800085067749, 2.300800085067749, 2.300800085067749, 2.300800085067749, 2.300800085067749, 2.300800085067749, 2.300800085067749, 2.300800085067749, 2.3006999492645264, 2.3006999492645264, 2.3006999492645264, 2.3006999492645264, 2.3006999492645264, 2.3006999492645264, 2.3006999492645264, 2.3006999492645264, 2.3006999492645264, 2.3006999492645264, 2.3006999492645264, 2.3006999492645264, 2.3006999492645264, 2.3006999492645264, 1.9563000202178955, 2.0525999069213867, 2.037100076675415, 2.0367000102996826, 0.09319999814033508, 1.0678000450134277, 2.336899995803833, 2.336899995803833, 2.336899995803833, 2.336899995803833, 2.336899995803833, 2.3368000984191895, 2.3368000984191895, 2.3368000984191895, 2.3368000984191895, 2.3368000984191895, 2.3368000984191895, 2.3368000984191895, 2.3368000984191895, 2.3368000984191895, 2.3368000984191895, 2.3368000984191895, 2.336699962615967, 2.336699962615967, 2.336699962615967, 2.336699962615967, 2.336699962615967, 2.336699962615967, 2.336699962615967, 2.336699962615967, 2.336699962615967, 2.336699962615967, 2.336699962615967, 2.336699962615967, 2.336699962615967, 2.336699962615967, 2.3364999294281006, 2.0097999572753906, 1.9700000286102295, 2.011699914932251, 1.6740000247955322, 1.4206000566482544, 2.382200002670288, 2.382200002670288, 2.382200002670288, 2.382200002670288, 2.3821001052856445, 2.3821001052856445, 2.3821001052856445, 2.3821001052856445, 2.3821001052856445, 2.3821001052856445, 2.3821001052856445, 2.3821001052856445, 2.3821001052856445, 2.3821001052856445, 2.3821001052856445, 2.3821001052856445, 2.381999969482422, 2.381999969482422, 2.381999969482422, 2.381999969482422, 2.381999969482422, 2.381999969482422, 2.381999969482422, 2.381999969482422, 2.381999969482422, 2.3819000720977783, 2.3819000720977783, 2.3819000720977783, 2.3819000720977783, 2.3819000720977783, 2.376699924468994, 2.067699909210205, 2.033600091934204, 2.037100076675415, 2.14490008354187, 1.899399995803833, 1.9218000173568726, 1.0422999858856201, 2.434000015258789, 2.4339001178741455, 2.4339001178741455, 2.4339001178741455, 2.4339001178741455, 2.4339001178741455, 2.4339001178741455, 2.4339001178741455, 2.4339001178741455, 2.4339001178741455, 2.4339001178741455, 2.4339001178741455, 2.433799982070923, 2.433799982070923, 2.433799982070923, 2.433799982070923, 2.433799982070923, 2.433799982070923, 2.4337000846862793, 2.4337000846862793, 2.4337000846862793, 2.4337000846862793, 2.4337000846862793, 2.4337000846862793, 2.4337000846862793, 2.4337000846862793, 2.4337000846862793, 2.4337000846862793, 2.4337000846862793, 2.4335999488830566, 2.4316999912261963, 2.179800033569336, 2.0216000080108643, 2.0325000286102295, 2.1273000240325928, 1.9972000122070312, 1.2253999710083008, 2.031599998474121, 1.892899990081787, 0.31119999289512634, 2.453000068664551, 2.453000068664551, 2.453000068664551, 2.453000068664551, 2.453000068664551, 2.453000068664551, 2.453000068664551, 2.452899932861328, 2.452899932861328, 2.452899932861328, 2.452899932861328, 2.452899932861328, 2.452899932861328, 2.452899932861328, 2.4528000354766846, 2.4528000354766846, 2.4528000354766846, 2.4528000354766846, 2.4528000354766846, 2.4528000354766846, 2.4528000354766846, 2.4528000354766846, 2.452699899673462, 2.452699899673462, 2.452699899673462, 2.452699899673462, 2.452699899673462, 2.452699899673462, 2.4526000022888184, 2.4526000022888184, 2.4481000900268555, 2.436800003051758, 2.2204999923706055, 2.165800094604492, 2.047100067138672, 1.7199000120162964, 2.037899971008301, 0.25429999828338623, 1.1471999883651733], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -4.234899997711182, -4.305200099945068, -4.308800220489502, -4.354700088500977, -4.3531999588012695, -4.498000144958496, -4.533400058746338, -4.545899868011475, -4.583099842071533, -4.624300003051758, -4.739299774169922, -4.763400077819824, -4.887899875640869, -4.956299781799316, -5.011000156402588, -5.038599967956543, -5.074699878692627, -5.114699840545654, -5.116399765014648, -5.150899887084961, -5.197999954223633, -5.224299907684326, -5.253399848937988, -5.350399971008301, -5.349599838256836, -5.187699794769287, -5.492700099945068, -5.498799800872803, -5.507299900054932, -5.518099784851074, -4.634500026702881, -4.885200023651123, -5.186500072479248, -5.232600212097168, -4.945300102233887, -4.228400230407715, -4.532899856567383, -4.57289981842041, -4.579100131988525, -5.000999927520752, -4.969600200653076, -5.070799827575684, -4.954100131988525, -3.615499973297119, -3.677799940109253, -3.6928000450134277, -4.027900218963623, -4.037700176239014, -4.064799785614014, -4.325099945068359, -4.426799774169922, -4.45959997177124, -4.465700149536133, -4.5472002029418945, -3.5069000720977783, -4.559100151062012, -4.695000171661377, -4.912199974060059, -4.990699768066406, -4.992499828338623, -5.013599872589111, -5.0279998779296875, -5.052299976348877, -5.061999797821045, -5.067800045013428, -4.644700050354004, -5.164400100708008, -5.1707000732421875, -5.179900169372559, -5.240600109100342, -5.242499828338623, -5.281300067901611, -5.285799980163574, -3.8759000301361084, -4.2789998054504395, -4.156899929046631, -3.4839000701904297, -4.852700233459473, -4.785299777984619, -4.866600036621094, -4.828800201416016, -4.98960018157959, -5.016300201416016, -3.859499931335449, -4.010700225830078, -4.044099807739258, -3.927000045776367, -4.147799968719482, -4.366799831390381, -4.366700172424316, -4.410399913787842, -4.4257001876831055, -4.460599899291992, -4.464700222015381, -4.529600143432617, -4.501999855041504, -4.662199974060059, -4.723800182342529, -4.734300136566162, -4.7565999031066895, -4.7891998291015625, -4.82420015335083, -4.8902997970581055, -5.007599830627441, -5.073400020599365, -5.09499979019165, -5.098599910736084, -5.105100154876709, -5.113100051879883, -5.128499984741211, -5.167600154876709, -5.180699825286865, -5.184100151062012, -4.64709997177124, -4.535999774932861, -4.401500225067139, -4.653299808502197, -4.625800132751465, -5.065000057220459, -4.979499816894531, -4.252900123596191, -4.26170015335083, -4.3769001960754395, -4.452300071716309, -4.491700172424316, -4.5528998374938965, -4.609099864959717, -4.626100063323975, -4.636099815368652, -4.557799816131592, -4.723400115966797, -4.759799957275391, -4.772600173950195, -4.8343000411987305, -4.877399921417236, -4.936500072479248, -4.938000202178955, -4.957799911499023, -4.965199947357178, -4.979800224304199, -5.0030999183654785, -5.0055999755859375, -5.0142998695373535, -5.0233001708984375, -5.0721001625061035, -5.074999809265137, -5.102700233459473, -5.103499889373779, -5.138599872589111, -5.145599842071533, -4.007800102233887, -4.521699905395508, -4.69890022277832, -4.967800140380859, -4.74399995803833, -4.929100036621094, -4.532100200653076, -4.967599868774414, -3.4983999729156494, -3.9368999004364014, -4.23859977722168, -4.241099834442139, -4.265600204467773, -4.275599956512451, -4.458899974822998, -4.5258002281188965, -4.549600124359131, -4.55049991607666, -4.594900131225586, -4.571000099182129, -4.693299770355225, -4.7459001541137695, -4.80210018157959, -4.853499889373779, -4.889800071716309, -4.897799968719482, -4.912499904632568, -4.963799953460693, -5.0040998458862305, -5.093200206756592, -5.233399868011475, -5.249300003051758, -5.254000186920166, -5.265100002288818, -5.2769999504089355, -5.368899822235107, -5.382500171661377, -5.386600017547607, -4.155600070953369, -4.752999782562256, -4.422999858856201, -4.054900169372559, -5.175300121307373, -2.8301000595092773, -3.0399999618530273, -3.6029999256134033, -4.039599895477295, -4.298799991607666, -4.481400012969971, -4.523099899291992, -4.623199939727783, -4.726099967956543, -4.932600021362305, -4.95550012588501, -4.996600151062012, -5.0030999183654785, -5.018899917602539, -5.135799884796143, -5.137800216674805, -5.162300109863281, -5.1915998458862305, -5.216400146484375, -5.2453999519348145, -5.248499870300293, -5.27239990234375, -5.281300067901611, -5.2941999435424805, -5.297800064086914, -5.3043999671936035, -5.312600135803223, -5.31689977645874, -5.352200031280518, -5.401299953460693, -4.506800174713135, -4.988399982452393, -4.990099906921387, -5.0995001792907715, -5.11269998550415, -5.249100208282471, -3.8350000381469727, -3.840100049972534, -3.96370005607605, -4.114699840545654, -4.156700134277344, -4.350299835205078, -4.629899978637695, -4.647600173950195, -4.678199768066406, -4.6894001960754395, -4.769700050354004, -4.786799907684326, -4.771900177001953, -4.8242998123168945, -4.852399826049805, -4.894400119781494, -4.982800006866455, -4.995299816131592, -5.0457000732421875, -5.047299861907959, -5.054800033569336, -5.087800025939941, -5.091599941253662, -5.1194000244140625, -5.1234002113342285, -5.158999919891357, -5.164400100708008, -5.207900047302246, -5.256800174713135, -5.260200023651123, -3.970900058746338, -4.593900203704834, -4.530700206756592, -4.752999782562256, -4.797299861907959, -4.954100131988525, -3.334700107574463, -3.9955999851226807, -4.027599811553955, -4.150899887084961, -4.468299865722656, -4.494999885559082, -4.54010009765625, -4.6072998046875, -4.6371002197265625, -4.660799980163574, -4.708600044250488, -4.724100112915039, -4.821499824523926, -4.847599983215332, -4.921500205993652, -4.940899848937988, -5.002299785614014, -5.0121002197265625, -5.069200038909912, -5.107800006866455, -5.136199951171875, -5.188899993896484, -5.192999839782715, -5.24459981918335, -5.246500015258789, -5.3125, -5.320899963378906, -5.343200206756592, -5.345200061798096, -5.371200084686279, -5.0167999267578125, -4.398399829864502, -4.84689998626709, -4.853600025177002, -5.011899948120117, -4.9008002281188965, -4.955900192260742, -5.00439977645874, -3.4595999717712402, -3.7769999504089355, -3.8691999912261963, -4.2993998527526855, -4.327099800109863, -4.369699954986572, -4.479000091552734, -4.497799873352051, -4.520699977874756, -4.5391998291015625, -4.543300151824951, -4.576099872589111, -4.80019998550415, -4.820499897003174, -4.844900131225586, -4.931399822235107, -5.015699863433838, -5.028900146484375, -5.122399806976318, -5.1778998374938965, -5.19290018081665, -5.200900077819824, -5.232500076293945, -5.244200229644775, -5.253399848937988, -5.2642998695373535, -5.331200122833252, -5.331299781799316, -5.334199905395508, -5.360199928283691, -4.730899810791016, -4.118299961090088, -3.940200090408325, -4.528200149536133, -4.718299865722656, -4.633699893951416, -3.9493000507354736, -5.082099914550781, -5.081200122833252, -4.894700050354004, -3.154400110244751, -3.338399887084961, -3.383500099182129, -3.6229000091552734, -3.8478000164031982, -3.907900094985962, -4.030700206756592, -4.184299945831299, -4.222799777984619, -4.239999771118164, -4.386899948120117, -4.4197001457214355, -4.514599800109863, -4.657100200653076, -4.800000190734863, -4.818600177764893, -4.930600166320801, -5.014900207519531, -5.022200107574463, -5.0279998779296875, -5.033699989318848, -5.048999786376953, -5.129899978637695, -5.231500148773193, -5.241600036621094, -5.279699802398682, -5.3059000968933105, -5.359899997711182, -5.418900012969971, -5.4394001960754395, -4.709099769592285, -4.890100002288818, -4.523799896240234, -4.7947998046875, -4.806300163269043, -4.709700107574463, -5.059199810028076, -4.920400142669678, -5.006700038909912]}, \"token.table\": {\"Topic\": [1, 10, 10, 2, 5, 1, 4, 10, 1, 5, 2, 10, 9, 7, 2, 8, 9, 5, 2, 9, 10, 9, 2, 9, 6, 6, 10, 8, 3, 9, 1, 10, 6, 7, 6, 7, 9, 9, 3, 2, 10, 9, 5, 10, 7, 1, 3, 1, 4, 5, 7, 8, 9, 10, 8, 7, 8, 8, 1, 2, 10, 10, 1, 7, 1, 4, 2, 10, 8, 1, 2, 10, 2, 3, 4, 5, 6, 7, 8, 10, 9, 4, 10, 5, 4, 3, 6, 4, 3, 7, 9, 1, 1, 4, 4, 7, 8, 5, 1, 5, 9, 2, 2, 7, 6, 8, 3, 1, 10, 5, 7, 9, 3, 4, 5, 2, 1, 4, 5, 1, 2, 6, 3, 9, 10, 2, 2, 1, 3, 10, 1, 7, 8, 4, 4, 6, 9, 6, 10, 2, 8, 4, 3, 6, 1, 2, 2, 4, 3, 5, 8, 5, 7, 3, 6, 4, 7, 2, 4, 4, 1, 4, 4, 6, 9, 6, 8, 5, 8, 3, 6, 10, 7, 8, 6, 8, 3, 5, 6, 9, 8, 1, 4, 5, 1, 2, 5, 10, 1, 4, 5, 2, 4, 1, 5, 8, 9, 3, 3, 5, 3, 7, 10, 1, 2, 3, 4, 6, 7, 9, 10, 2, 4, 7, 4, 7, 6, 9, 6, 1, 7, 1, 7, 10, 7, 2, 7, 9, 5, 1, 6, 7, 5, 2, 10, 3, 4, 4, 6, 6, 9, 4, 9, 8, 10, 2, 7, 5, 8, 2, 3, 6, 5, 8, 9, 3, 8, 5, 8, 3, 5, 8, 1, 3, 3, 8, 5, 8, 10, 3, 1, 5, 4, 8, 5, 9, 9, 3, 4, 4, 4, 5, 7, 6, 8, 8, 2, 4, 6, 1, 8, 1, 8, 3, 3, 7, 9, 1, 9, 9, 6, 10, 3, 6, 9, 1, 7, 7, 1, 1, 9, 8, 5, 10, 10, 5, 1, 4, 8, 7, 6, 3, 2, 2, 9, 10, 8, 5, 9, 10, 4, 5, 3, 1, 2, 9, 10, 9, 6, 3, 1, 2, 4, 9, 1, 5, 10, 6, 8, 10, 10, 3, 6, 9, 1, 4, 5, 5, 1, 2, 7, 1, 7, 1, 7, 4, 6, 5, 6, 3, 2, 9, 10, 3, 1, 10, 3, 4, 8, 1, 2, 4, 5, 6, 7, 8, 9, 1, 2, 4, 5, 9, 3, 7, 8, 7, 5, 5, 10, 4, 5, 9, 3, 9, 8, 5, 6, 7, 7, 4, 1, 2, 8, 4, 2, 4, 5, 3, 4, 3, 1, 9, 1, 2, 4, 7, 9, 9, 10, 10, 1, 7, 10, 3, 8, 6, 2, 2, 2, 5, 6, 6, 4, 2, 4, 7, 1, 6, 1, 1, 8, 9, 1, 2, 2, 10, 1, 6, 8, 3, 4, 2, 9, 2, 2, 2, 10, 9, 1, 5, 8, 3, 5, 8, 4, 3, 3, 3, 7, 1, 7, 7, 6, 2, 9, 3, 4, 6, 10, 6, 10, 1, 5, 10, 8], \"Freq\": [0.9977968335151672, 0.0020615635439753532, 0.9997956156730652, 0.8909550905227661, 0.10895221680402756, 0.09343202412128448, 0.9064793586730957, 0.9999610185623169, 0.9997960329055786, 0.9997584223747253, 0.9999012351036072, 0.9995381236076355, 0.9998772740364075, 0.9997972249984741, 0.005168613512068987, 0.994502067565918, 0.9996064901351929, 0.9996895790100098, 0.6389473676681519, 0.2529495358467102, 0.10806887596845627, 0.9998371601104736, 0.3379597067832947, 0.6620465517044067, 0.9999862313270569, 0.9999780654907227, 0.9998995065689087, 0.9996058940887451, 0.3306259214878082, 0.6693159341812134, 0.01593037322163582, 0.9839849472045898, 0.9998670816421509, 0.9996368288993835, 0.9998265504837036, 0.9997484087944031, 0.9998883008956909, 0.9998769760131836, 0.9998767375946045, 0.9996257424354553, 0.999870240688324, 0.9995432496070862, 0.9997796416282654, 0.9996596574783325, 0.9998869895935059, 0.2819494605064392, 0.7179718613624573, 0.24669106304645538, 0.08956415951251984, 0.2796015441417694, 0.06857112795114517, 0.18921232223510742, 0.09543120861053467, 0.030893677845597267, 0.9998467564582825, 0.9999046325683594, 0.999836802482605, 0.9995418787002563, 0.9997778534889221, 0.9998863339424133, 0.9998162388801575, 0.999679446220398, 0.9997680187225342, 0.9997962713241577, 0.2559589445590973, 0.7440012097358704, 0.9998792409896851, 0.00011459933739388362, 0.9998473525047302, 0.21354876458644867, 0.5561253428459167, 0.23011599481105804, 0.05711420997977257, 0.3203906714916229, 0.12808947265148163, 0.12082494050264359, 0.08650631457567215, 0.14395453035831451, 0.10512688755989075, 0.03799264132976532, 0.9998361468315125, 0.99985271692276, 0.9995293617248535, 0.9999134540557861, 0.9998413324356079, 0.9997565150260925, 0.9998447299003601, 0.9998055696487427, 0.5999114513397217, 0.40001416206359863, 0.9998472332954407, 0.9999224543571472, 0.9999411106109619, 0.9997064471244812, 0.9998525977134705, 0.9998744130134583, 0.9996305108070374, 0.9996246099472046, 0.9998981952667236, 0.999916672706604, 0.9995865821838379, 0.9999523162841797, 0.7711594700813293, 0.22880835831165314, 0.9997437596321106, 0.999660074710846, 0.9996501803398132, 0.9995250105857849, 0.9998077154159546, 0.9996461868286133, 0.9997727274894714, 0.9998459219932556, 0.2343350052833557, 0.5767553448677063, 0.1888490915298462, 0.999914288520813, 0.9998381733894348, 0.9998419880867004, 0.9997396469116211, 0.9996519088745117, 0.9999417662620544, 0.9998749494552612, 0.9998676776885986, 0.9996058940887451, 0.9999290108680725, 0.9999071955680847, 0.9998952746391296, 0.9993228912353516, 0.0006426514009945095, 0.999819278717041, 0.9998255372047424, 0.9997831583023071, 0.9999397993087769, 0.9997753500938416, 0.9998384118080139, 0.9996436238288879, 0.9998021721839905, 0.9996190071105957, 0.9995774626731873, 0.9998865127563477, 0.9997737407684326, 0.9998488426208496, 0.9998788237571716, 0.9998782873153687, 0.9999516606330872, 0.9997072219848633, 0.9997004866600037, 0.9999361634254456, 0.12753021717071533, 0.8723851442337036, 0.999740719795227, 0.9997051358222961, 0.9995948076248169, 0.2914983928203583, 0.7084214091300964, 0.9997990131378174, 0.9998923540115356, 0.9999045133590698, 0.9997806549072266, 0.9998970031738281, 0.9997441172599792, 0.999762237071991, 0.2622685134410858, 0.0017578318947926164, 0.7358283996582031, 0.9995722770690918, 0.9997708797454834, 0.9999546408653259, 0.9998138546943665, 0.0038976173382252455, 0.32952582836151123, 0.6663153767585754, 0.9998230934143066, 0.9997959733009338, 0.2111501693725586, 0.788693904876709, 0.9998140335083008, 0.9999535083770752, 0.9997618198394775, 0.999707043170929, 0.9997680187225342, 0.9429501295089722, 0.05668458715081215, 0.000206878044991754, 0.9997161030769348, 0.9998096823692322, 0.9997947812080383, 0.9996924996376038, 0.15893204510211945, 0.18342222273349762, 0.6575762629508972, 0.9997109174728394, 0.00017874324112199247, 0.5699683427810669, 0.15900851786136627, 0.12588895857334137, 0.14513644576072693, 0.9998915791511536, 0.9996516108512878, 0.9999008774757385, 0.9998160004615784, 0.9998944997787476, 0.999801516532898, 0.17955844104290009, 0.2218955159187317, 0.09004328399896622, 0.0051050931215286255, 0.07736856490373611, 0.0913635641336441, 0.06372564285993576, 0.2709220051765442, 0.9998005032539368, 0.30705904960632324, 0.6928128600120544, 0.9998569488525391, 0.9999140501022339, 0.9996639490127563, 0.9997472763061523, 0.9998576641082764, 0.9999234676361084, 0.9996944069862366, 0.9998378753662109, 0.9998951554298401, 0.9997566342353821, 0.9999122619628906, 0.0009660884388722479, 0.7209780216217041, 0.2779574394226074, 0.999734103679657, 0.30318912863731384, 0.18145409226417542, 0.5153489708900452, 0.9998798966407776, 0.5195127129554749, 0.48043328523635864, 0.9996673464775085, 0.999704897403717, 0.9998424649238586, 0.9997896552085876, 0.9997169971466064, 0.9995437264442444, 0.9997005462646484, 0.9999726414680481, 0.9998719096183777, 0.999748170375824, 0.9997833371162415, 0.999788761138916, 0.26979026198387146, 0.7301056385040283, 0.9996411204338074, 0.23201772570610046, 0.76775062084198, 0.21791045367717743, 0.7081629633903503, 0.0737401619553566, 0.9997471570968628, 0.9997704029083252, 0.9999563694000244, 0.9998753070831299, 0.9999435544013977, 0.9996874332427979, 0.9998727440834045, 0.25953370332717896, 0.740354061126709, 0.9998770952224731, 0.9998035430908203, 0.9998895525932312, 0.9994583129882812, 0.9998679757118225, 0.9999501705169678, 0.9998530745506287, 0.9998510479927063, 0.7861760854721069, 0.21362970769405365, 0.9996992349624634, 0.9998924136161804, 0.9995645880699158, 0.9734140634536743, 0.02638224884867668, 0.9996145367622375, 0.7642894983291626, 0.23548929393291473, 0.9999585747718811, 0.9997228980064392, 0.999589741230011, 0.9998193979263306, 0.9999614953994751, 0.9997014999389648, 0.9997664093971252, 0.7380676865577698, 0.2618618309497833, 0.38286393880844116, 0.6170510053634644, 0.9998951554298401, 0.0004105022526346147, 0.9995729327201843, 0.9998767971992493, 0.9998605251312256, 0.9998909831047058, 0.9995125532150269, 0.9996408224105835, 0.9993906617164612, 0.7086089849472046, 0.29131704568862915, 0.9997323751449585, 0.9965234398841858, 0.003496573306620121, 0.999806821346283, 0.9997598528862, 0.9998557567596436, 0.9997570514678955, 0.99981290102005, 0.9996396899223328, 0.9998655915260315, 0.9999154806137085, 0.9999011158943176, 0.5899280905723572, 0.257392019033432, 0.15260158479213715, 0.999864399433136, 0.9997369050979614, 0.9996619820594788, 0.999656081199646, 0.5904156565666199, 0.29862990975379944, 0.11096125096082687, 0.9996125102043152, 0.9998473525047302, 0.6686080694198608, 0.3312011659145355, 0.9999382495880127, 0.9998459815979004, 0.9998952746391296, 0.9996403455734253, 0.0001972131576621905, 0.24927742779254913, 0.7503960132598877, 0.9998561143875122, 0.9998739957809448, 0.9997175931930542, 0.999741792678833, 0.002846898976713419, 0.22154974937438965, 0.775576651096344, 0.14966033399105072, 0.8500852584838867, 0.9999385476112366, 0.7800522446632385, 0.21969690918922424, 0.999455988407135, 0.9996479749679565, 0.7639912962913513, 0.23600801825523376, 0.9997851252555847, 0.7088170051574707, 0.15965235233306885, 0.13137201964855194, 0.9996265172958374, 0.1810997575521469, 0.630416750907898, 0.18829144537448883, 0.9998657703399658, 0.9998090863227844, 0.9997486472129822, 0.9997532963752747, 0.9996915459632874, 0.9998288154602051, 0.9998155236244202, 0.9998340606689453, 0.9998373985290527, 0.999921441078186, 0.9995725154876709, 0.9999293684959412, 0.9996943473815918, 0.9999376535415649, 0.9997941851615906, 0.9997947216033936, 0.2941698133945465, 0.705570638179779, 0.15410612523555756, 0.0006821873830631375, 0.20260964334011078, 0.3189567029476166, 0.1099344938993454, 0.09328912198543549, 0.000716296723112464, 0.11968977004289627, 0.5772390961647034, 0.12096362560987473, 0.12654924392700195, 0.13702228665351868, 0.03813932463526726, 0.9998734593391418, 0.9998010993003845, 0.9999186396598816, 0.9998170137405396, 0.9998839497566223, 0.20745296776294708, 0.7924830317497253, 0.7243685126304626, 0.27542588114738464, 0.9998395442962646, 0.9998035430908203, 0.9998935461044312, 0.9995015263557434, 0.999692440032959, 0.9996241331100464, 0.9998565912246704, 0.9998990893363953, 0.9998857378959656, 0.999844491481781, 0.9997350573539734, 0.9999027252197266, 0.9997898936271667, 0.9996793270111084, 0.99991774559021, 0.9998287558555603, 0.2214362919330597, 0.7783408164978027, 0.9999070167541504, 0.0021721019875258207, 0.997718870639801, 0.5903118252754211, 0.11509685963392258, 0.002202108269557357, 0.11186710000038147, 0.18057288229465485, 0.9998518824577332, 0.9996810555458069, 0.9999654293060303, 0.6103554964065552, 0.21374103426933289, 0.17587676644325256, 0.9999352097511292, 0.9999062418937683, 0.9997332692146301, 0.9997002482414246, 0.9996469616889954, 0.742027759552002, 0.25790485739707947, 0.9995138645172119, 0.9998630285263062, 0.9996652007102966, 0.999681293964386, 0.9996890425682068, 0.99977046251297, 0.9999419450759888, 0.9996349215507507, 0.9995148777961731, 0.4171363115310669, 0.0005996209802106023, 0.5820320844650269, 0.999671995639801, 0.9998813271522522, 0.9996718764305115, 0.9999588131904602, 0.9998567700386047, 0.7680597305297852, 0.23185117542743683, 0.9997876286506653, 0.9997200965881348, 0.3538341224193573, 0.6461195349693298, 0.9998978972434998, 0.9998500347137451, 0.3395041525363922, 0.6602473855018616, 0.9998828768730164, 0.18805384635925293, 0.18079307675361633, 0.6309605836868286, 0.9997940063476562, 0.9999538660049438, 0.9998998045921326, 0.9998969435691833, 0.9996374845504761, 0.9998613595962524, 0.27755531668663025, 0.7224220037460327, 0.9998923540115356, 0.9999231696128845, 0.9996578693389893, 0.9998853206634521, 0.99967360496521, 0.0002485513687133789, 0.9998798370361328, 0.9998804330825806, 0.9999550580978394, 0.9999473094940186, 0.004801192786544561, 0.9950472712516785, 0.9988456964492798, 0.0005578584969043732, 0.0002789292484521866, 0.99981689453125], \"Term\": [\"aborigin\", \"aborigin\", \"abus\", \"accus\", \"accus\", \"action\", \"action\", \"adelaid\", \"age\", \"alan\", \"alleg\", \"america\", \"amid\", \"andrew\", \"anti\", \"anti\", \"apologis\", \"appeal\", \"arrest\", \"arrest\", \"arrest\", \"assault\", \"attack\", \"attack\", \"australia\", \"australian\", \"babi\", \"ban\", \"bank\", \"bank\", \"beach\", \"beach\", \"beat\", \"begin\", \"best\", \"black\", \"bodi\", \"bomb\", \"break\", \"brief\", \"brisban\", \"british\", \"budget\", \"burn\", \"bushfir\", \"busi\", \"busi\", \"call\", \"call\", \"call\", \"call\", \"call\", \"call\", \"call\", \"campaign\", \"canberra\", \"cancer\", \"candid\", \"care\", \"case\", \"catch\", \"celebr\", \"centr\", \"challeng\", \"chang\", \"chang\", \"charg\", \"charg\", \"check\", \"child\", \"child\", \"child\", \"china\", \"china\", \"china\", \"china\", \"china\", \"china\", \"china\", \"china\", \"chines\", \"christma\", \"church\", \"claim\", \"climat\", \"club\", \"coach\", \"coal\", \"coast\", \"coast\", \"comment\", \"commiss\", \"communiti\", \"compani\", \"concern\", \"continu\", \"control\", \"convict\", \"council\", \"countri\", \"coupl\", \"court\", \"crash\", \"crash\", \"cricket\", \"crime\", \"cyclon\", \"dairi\", \"darwin\", \"data\", \"david\", \"dead\", \"deal\", \"deal\", \"deal\", \"death\", \"debat\", \"delay\", \"deni\", \"develop\", \"die\", \"disabl\", \"dollar\", \"domest\", \"donald\", \"drive\", \"driver\", \"drought\", \"drought\", \"drug\", \"drum\", \"east\", \"elect\", \"emerg\", \"energi\", \"england\", \"expect\", \"explain\", \"extend\", \"face\", \"facebook\", \"fail\", \"fall\", \"farm\", \"farmer\", \"fatal\", \"father\", \"fear\", \"feder\", \"feder\", \"femal\", \"fiji\", \"film\", \"final\", \"final\", \"financ\", \"find\", \"fire\", \"fish\", \"flood\", \"food\", \"footag\", \"forc\", \"forc\", \"forc\", \"franc\", \"friday\", \"fund\", \"futur\", \"game\", \"game\", \"game\", \"girl\", \"give\", \"go\", \"go\", \"gold\", \"govern\", \"grand\", \"great\", \"green\", \"group\", \"group\", \"group\", \"grow\", \"guilti\", \"hand\", \"head\", \"health\", \"health\", \"health\", \"hear\", \"hear\", \"help\", \"help\", \"help\", \"help\", \"high\", \"hill\", \"histori\", \"hit\", \"hobart\", \"hold\", \"home\", \"home\", \"home\", \"home\", \"home\", \"home\", \"home\", \"home\", \"hong\", \"hospit\", \"hospit\", \"hour\", \"hous\", \"human\", \"hunt\", \"india\", \"indigen\", \"indonesia\", \"industri\", \"inquest\", \"insid\", \"interview\", \"investig\", \"investig\", \"investig\", \"islam\", \"island\", \"island\", \"island\", \"issu\", \"jail\", \"jail\", \"jam\", \"job\", \"john\", \"johnson\", \"juli\", \"june\", \"kid\", \"kill\", \"killer\", \"know\", \"kong\", \"korea\", \"labor\", \"labor\", \"lawyer\", \"lead\", \"lead\", \"leader\", \"leader\", \"leader\", \"leagu\", \"liber\", \"life\", \"light\", \"live\", \"local\", \"long\", \"look\", \"look\", \"lose\", \"love\", \"make\", \"manag\", \"mark\", \"market\", \"mayor\", \"media\", \"meet\", \"meet\", \"mental\", \"michael\", \"militari\", \"million\", \"million\", \"mine\", \"minist\", \"minist\", \"miss\", \"monday\", \"money\", \"morrison\", \"murder\", \"murray\", \"music\", \"nation\", \"nation\", \"need\", \"need\", \"news\", \"north\", \"north\", \"number\", \"nurs\", \"offic\", \"offici\", \"olymp\", \"onlin\", \"open\", \"open\", \"oper\", \"outback\", \"outback\", \"pacif\", \"parent\", \"park\", \"parliament\", \"parti\", \"paul\", \"peopl\", \"perth\", \"peter\", \"plan\", \"plan\", \"plan\", \"plane\", \"play\", \"player\", \"plead\", \"polic\", \"polic\", \"polic\", \"polici\", \"polit\", \"port\", \"port\", \"power\", \"presid\", \"price\", \"princ\", \"prison\", \"prison\", \"prison\", \"program\", \"project\", \"properti\", \"protect\", \"protest\", \"protest\", \"protest\", \"public\", \"public\", \"queensland\", \"race\", \"race\", \"rape\", \"rat\", \"record\", \"record\", \"refuge\", \"region\", \"region\", \"region\", \"reject\", \"releas\", \"releas\", \"releas\", \"remot\", \"rescu\", \"research\", \"result\", \"retir\", \"return\", \"reveal\", \"right\", \"rise\", \"road\", \"rock\", \"royal\", \"rugbi\", \"rural\", \"russia\", \"sale\", \"save\", \"save\", \"say\", \"say\", \"say\", \"say\", \"say\", \"say\", \"say\", \"say\", \"school\", \"school\", \"school\", \"school\", \"school\", \"scott\", \"search\", \"season\", \"second\", \"senat\", \"sentenc\", \"sentenc\", \"servic\", \"servic\", \"sexual\", \"share\", \"shoot\", \"shorten\", \"slam\", \"smith\", \"social\", \"south\", \"speak\", \"sport\", \"stab\", \"state\", \"station\", \"steal\", \"stop\", \"stori\", \"storm\", \"storm\", \"street\", \"strike\", \"strike\", \"student\", \"student\", \"student\", \"student\", \"student\", \"suicid\", \"super\", \"sydney\", \"take\", \"take\", \"take\", \"tasmania\", \"tasmanian\", \"team\", \"teen\", \"teenag\", \"tell\", \"tell\", \"territori\", \"test\", \"thousand\", \"threat\", \"threaten\", \"thursday\", \"time\", \"tour\", \"tourism\", \"town\", \"town\", \"town\", \"tree\", \"trial\", \"truck\", \"trump\", \"tuesday\", \"turn\", \"turn\", \"turnbul\", \"union\", \"victim\", \"victim\", \"victoria\", \"victorian\", \"video\", \"video\", \"violenc\", \"vote\", \"vote\", \"vote\", \"wall\", \"warn\", \"water\", \"weather\", \"wednesday\", \"week\", \"west\", \"west\", \"western\", \"white\", \"william\", \"win\", \"woman\", \"woman\", \"women\", \"worker\", \"world\", \"year\", \"young\", \"young\", \"youth\", \"youth\", \"youth\", \"zealand\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [8, 4, 6, 5, 10, 3, 1, 7, 2, 9]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el69891403026246480728488048203\", ldavis_el69891403026246480728488048203_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el69891403026246480728488048203\", ldavis_el69891403026246480728488048203_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el69891403026246480728488048203\", ldavis_el69891403026246480728488048203_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "7     -0.187451  0.084184       1        1  11.980048\n",
       "3      0.203714  0.189695       2        1  10.884151\n",
       "5      0.052967 -0.287178       3        1  10.436221\n",
       "4     -0.170358  0.026991       4        1  10.327317\n",
       "9     -0.208822  0.080443       5        1  10.089189\n",
       "2      0.095006 -0.282747       6        1  10.015703\n",
       "0      0.055003 -0.050999       7        1   9.662029\n",
       "6     -0.204657 -0.004956       8        1   9.233929\n",
       "1      0.163490  0.180418       9        1   8.768364\n",
       "8      0.201108  0.064149      10        1   8.603042, topic_info=     Category          Freq        Term         Total  loglift  logprob\n",
       "37    Default  31594.000000   australia  31594.000000  30.0000  30.0000\n",
       "16    Default  25612.000000  australian  25612.000000  29.0000  29.0000\n",
       "6665  Default  19623.000000       trump  19623.000000  28.0000  28.0000\n",
       "761   Default  17587.000000       elect  17587.000000  27.0000  27.0000\n",
       "236   Default  30244.000000       polic  30244.000000  26.0000  26.0000\n",
       "...       ...           ...         ...           ...      ...      ...\n",
       "1495  Topic10   3761.119629        game   5644.474121   2.0471  -4.8063\n",
       "328   Topic10   4142.789062        jail   8623.465820   1.7199  -4.7097\n",
       "2679  Topic10   2920.907959       video   4424.099121   2.0379  -5.0592\n",
       "236   Topic10   3355.546143       polic  30244.792969   0.2543  -4.9204\n",
       "109   Topic10   3078.295898        home  11361.203125   1.1472  -5.0067\n",
       "\n",
       "[412 rows x 6 columns], token_table=      Topic      Freq      Term\n",
       "term                           \n",
       "322       1  0.997797  aborigin\n",
       "322      10  0.002062  aborigin\n",
       "1891     10  0.999796      abus\n",
       "211       2  0.890955     accus\n",
       "211       5  0.108952     accus\n",
       "...     ...       ...       ...\n",
       "356      10  0.995047     young\n",
       "114       1  0.998846     youth\n",
       "114       5  0.000558     youth\n",
       "114      10  0.000279     youth\n",
       "383       8  0.999817   zealand\n",
       "\n",
       "[495 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[8, 4, 6, 5, 10, 3, 1, 7, 2, 9])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the topics\n",
    "# import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "tic1 = datetime.now()\n",
    "pyLDAvis.enable_notebook()\n",
    "print(\"Time to enable pyLDAvis is:{}\\n\".format(datetime.now()-tic1))\n",
    "\n",
    "tic1 = datetime.now()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, bow_corpus, dictionary)\n",
    "print(\"Time to prepare pyLDAvis is:{}\\n\\n\\n\".format(datetime.now()-tic1))\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate topics for a particular document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['decid', 'communiti', 'broadcast', 'licenc']\n"
     ]
    }
   ],
   "source": [
    "print(processed_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(bow_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.41999254), (1, 0.020004254), (2, 0.020004254), (3, 0.020004254), (4, 0.020004254), (5, 0.020004254), (6, 0.21994638), (7, 0.22003132), (8, 0.020004254), (9, 0.020004254)]\n"
     ]
    }
   ],
   "source": [
    "print(lda_model[bow_corpus[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = lda_model[bow_corpus[0]]\n",
    "print(\"Original list={}\".format(x))\n",
    "x = sorted(x, key=lambda tup: -1*tup[1])\n",
    "print(\"New list={}\".format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "top_topic_list = lda_model.top_topics(corpus=bow_corpus)\n",
    "print(len(top_topic_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([(0.030688798, 'polic'), (0.02998921, 'charg'), (0.026902873, 'court'), (0.025279678, 'death'), (0.024902096, 'murder'), (0.0207365, 'woman'), (0.01781122, 'die'), (0.017638607, 'face'), (0.017165683, 'alleg'), (0.01565655, 'crash'), (0.013857137, 'accus'), (0.013231679, 'trial'), (0.011952827, 'case'), (0.0115669845, 'guilti'), (0.011497213, 'victoria'), (0.010596322, 'road'), (0.01047113, 'driver'), (0.00961268, 'hear'), (0.009141206, 'victorian'), (0.008351579, 'arrest')], -4.761285740524762)\n"
     ]
    }
   ],
   "source": [
    "print(top_topic_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort the words in a document w.r.t. their dirichlet probability values in a descending manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.41999250650405884\t \n",
      "Topic: 0.022*\"hous\" + 0.021*\"south\" + 0.019*\"canberra\" + 0.019*\"north\" + 0.016*\"bushfir\" + 0.016*\"miss\" + 0.013*\"interview\" + 0.011*\"hospit\" + 0.010*\"investig\" + 0.010*\"search\"\n",
      "\n",
      "Score: 0.22003132104873657\t \n",
      "Topic: 0.015*\"nation\" + 0.014*\"farmer\" + 0.013*\"time\" + 0.013*\"rural\" + 0.013*\"council\" + 0.013*\"indigen\" + 0.011*\"commiss\" + 0.011*\"plan\" + 0.011*\"drum\" + 0.011*\"communiti\"\n",
      "\n",
      "Score: 0.21994638442993164\t \n",
      "Topic: 0.036*\"elect\" + 0.018*\"water\" + 0.018*\"state\" + 0.016*\"tasmanian\" + 0.012*\"labor\" + 0.011*\"liber\" + 0.011*\"morrison\" + 0.011*\"parti\" + 0.010*\"campaign\" + 0.010*\"give\"\n",
      "\n",
      "Score: 0.020004253834486008\t \n",
      "Topic: 0.031*\"kill\" + 0.023*\"shoot\" + 0.021*\"dead\" + 0.019*\"attack\" + 0.019*\"polic\" + 0.016*\"protest\" + 0.014*\"offic\" + 0.013*\"assault\" + 0.013*\"chines\" + 0.011*\"bodi\"\n",
      "\n",
      "Score: 0.020004253834486008\t \n",
      "Topic: 0.059*\"australia\" + 0.048*\"australian\" + 0.027*\"world\" + 0.018*\"test\" + 0.014*\"win\" + 0.011*\"farm\" + 0.011*\"final\" + 0.011*\"return\" + 0.010*\"beat\" + 0.009*\"cricket\"\n",
      "\n",
      "Score: 0.020004253834486008\t \n",
      "Topic: 0.031*\"polic\" + 0.030*\"charg\" + 0.027*\"court\" + 0.025*\"death\" + 0.025*\"murder\" + 0.021*\"woman\" + 0.018*\"die\" + 0.018*\"face\" + 0.017*\"alleg\" + 0.016*\"crash\"\n",
      "\n",
      "Score: 0.020004253834486008\t \n",
      "Topic: 0.018*\"chang\" + 0.014*\"speak\" + 0.014*\"power\" + 0.013*\"worker\" + 0.012*\"climat\" + 0.011*\"concern\" + 0.011*\"minist\" + 0.011*\"say\" + 0.011*\"john\" + 0.010*\"flood\"\n",
      "\n",
      "Score: 0.020004253834486008\t \n",
      "Topic: 0.021*\"market\" + 0.020*\"news\" + 0.018*\"women\" + 0.018*\"live\" + 0.016*\"tasmania\" + 0.013*\"high\" + 0.013*\"rise\" + 0.012*\"open\" + 0.012*\"price\" + 0.012*\"lose\"\n",
      "\n",
      "Score: 0.020004253834486008\t \n",
      "Topic: 0.043*\"trump\" + 0.035*\"year\" + 0.034*\"sydney\" + 0.027*\"queensland\" + 0.021*\"donald\" + 0.020*\"adelaid\" + 0.018*\"perth\" + 0.015*\"brisban\" + 0.015*\"peopl\" + 0.014*\"royal\"\n",
      "\n",
      "Score: 0.020004253834486008\t \n",
      "Topic: 0.030*\"govern\" + 0.020*\"warn\" + 0.017*\"say\" + 0.016*\"feder\" + 0.014*\"countri\" + 0.014*\"fund\" + 0.014*\"claim\" + 0.014*\"life\" + 0.012*\"health\" + 0.012*\"stori\"\n"
     ]
    }
   ],
   "source": [
    "for ind, score in sorted(lda_model[bow_corpus[0]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(ind, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate using Perplexity and coherence scores\n",
    "\n",
    "* Model perplexity and topic coherence provide a convenient measure to judge how good a given topic model is.\n",
    "* People in the NLP community prefer topic coherence score, in particular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -8.84560122283682\n",
      "Time to compute perplexity is 0:04:08.483101s\n",
      "\n",
      "Coherence Score:  0.2295940079410736\n",
      "Time to compute coherence score is 0:00:33.320570s\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Compute Perplexity\n",
    "tic1 = datetime.now()\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(bow_corpus))  # a measure of how good the model is. lower the better.\n",
    "tot_time = datetime.now() - tic1\n",
    "print(\"Time to compute perplexity is {}s\".format(tot_time))\n",
    "\n",
    "# Compute Coherence Score\n",
    "tic1 = datetime.now()\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_docs, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)\n",
    "tot_time = datetime.now() - tic1\n",
    "print(\"Time to compute coherence score is {}s\".format(tot_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance evaluation by classifying sample document using LDA TF-IDF model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize topic keywords\n",
    "\n",
    "* Each bubble on the left-hand side plot represents a topic. The larger the bubble, the more prevalent is that topic.\n",
    "\n",
    "* A good topic model will have fairly big, non-overlapping bubbles scattered throughout the chart instead of being clustered in one quadrant.\n",
    "\n",
    "* A model with too many topics, will typically have many overlaps, small sized bubbles clustered in one region of the chart.\n",
    "\n",
    "* Alright, if you move the cursor over one of the bubbles, the words and bars on the right-hand side will update. These words are the salient keywords that form the selected topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "# import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "tic1 = datetime.now()\n",
    "pyLDAvis.enable_notebook()\n",
    "print(\"Time to enable pyLDAvis is:{}s\\n\".format(datetime.now()-tic1))\n",
    "\n",
    "tic1 = datetime.now()\n",
    "vis = pyLDAvis.gensim.prepare(lda_tfidf_model, corpus_tfidf, dictionary, mds='mmds')\n",
    "print(\"Time to prepare pyLDAvis is:{}s\\n\\n\\n\".format(datetime.now()-tic1))\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate topics for a particular document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus_tfidf[4310], processed_docs[4310])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, score in sorted(lda_tfidf_model[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate using Perplexity and coherence scores\n",
    "\n",
    "* Model perplexity and topic coherence provide a convenient measure to judge how good a given topic model is.\n",
    "* People in the NLP community prefer topic coherence score, in particular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Compute Perplexity\n",
    "tic1 = datetime.now()\n",
    "print('\\nPerplexity: ', lda_tfidf_model.log_perplexity(corpus_tfidf))  # a measure of how good the model is. lower the better.\n",
    "tot_time = datetime.now() - tic1\n",
    "print(\"Time to compute perplexity is {}s\".format(tot_time))\n",
    "\n",
    "# Compute Coherence Score\n",
    "tic1 = datetime.now()\n",
    "coherence_model_lda = CoherenceModel(model=lda_tfidf_model, texts=processed_docs, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)\n",
    "tot_time = datetime.now() - tic1\n",
    "print(\"Time to compute coherence score is {}s\".format(tot_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find dominant topic for a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time of issuing command:2020-06-01 12:48:07.717466\n",
      "0.0% completed in time 0:00:00.009350\n",
      "0.843% completed in time 0:00:42.616524\n",
      "1.686% completed in time 0:01:27.484079\n",
      "2.529% completed in time 0:02:21.150710\n",
      "3.373% completed in time 0:03:30.126089\n",
      "4.216% completed in time 0:04:51.614676\n",
      "5.059% completed in time 0:06:02.301676\n",
      "5.902% completed in time 0:07:19.103093\n",
      "6.745% completed in time 0:08:42.268710\n",
      "7.588% completed in time 0:10:12.931465\n",
      "8.432% completed in time 0:11:47.699571\n",
      "9.275% completed in time 0:13:28.655612\n",
      "10.118% completed in time 0:15:15.303031\n",
      "10.961% completed in time 0:17:07.756615\n",
      "11.804% completed in time 0:19:20.061548\n",
      "12.647% completed in time 0:21:29.056208\n",
      "13.491% completed in time 0:23:40.696063\n",
      "14.334% completed in time 0:25:57.207963\n",
      "15.177% completed in time 0:28:21.280857\n",
      "16.02% completed in time 0:30:51.569331\n",
      "16.863% completed in time 0:33:27.586731\n",
      "17.706% completed in time 0:36:09.696563\n",
      "18.549% completed in time 0:38:56.927463\n",
      "19.393% completed in time 0:41:50.819940\n",
      "20.236% completed in time 0:44:53.796637\n",
      "21.079% completed in time 0:48:00.947873\n",
      "21.922% completed in time 0:51:18.788281\n",
      "22.765% completed in time 0:55:21.435620\n",
      "23.608% completed in time 0:58:58.928922\n",
      "24.452% completed in time 1:02:40.116458\n",
      "25.295% completed in time 1:06:20.717661\n",
      "26.138% completed in time 1:10:07.069355\n",
      "26.981% completed in time 1:13:59.683185\n",
      "27.824% completed in time 1:17:57.942007\n",
      "28.667% completed in time 1:22:01.603696\n",
      "29.511% completed in time 1:26:12.017439\n",
      "30.354% completed in time 1:30:28.108486\n",
      "31.197% completed in time 1:34:49.939838\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-7a2784e37f90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mtic1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Time of issuing command:{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtic1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mdf_topic_sents_keywords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_topics_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mldamodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbow_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocessed_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mtic1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-7a2784e37f90>\u001b[0m in \u001b[0;36mformat_topics_sentences\u001b[0;34m(ldamodel, corpus, texts)\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0mwp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mldamodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_topic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0mtopic_keywords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprop\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                 \u001b[0msent_topics_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_topics_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprop_topic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_keywords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(self, other, ignore_index, verify_integrity, sort)\u001b[0m\n\u001b[1;32m   7121\u001b[0m             \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7122\u001b[0m             \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7123\u001b[0;31m             \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7124\u001b[0m         )\n\u001b[1;32m   7125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    256\u001b[0m     )\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             new_data = concatenate_block_managers(\n\u001b[0;32m--> 473\u001b[0;31m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m             )\n\u001b[1;32m    475\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m   2048\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_uniform_join_units\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2049\u001b[0m             b = join_units[0].block.concat_same_type(\n\u001b[0;32m-> 2050\u001b[0;31m                 \u001b[0;34m[\u001b[0m\u001b[0mju\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mju\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjoin_units\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2051\u001b[0m             )\n\u001b[1;32m   2052\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mconcat_same_type\u001b[0;34m(self, to_concat, placement)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \"\"\"\n\u001b[1;32m    362\u001b[0m         values = self._concatenator(\n\u001b[0;32m--> 363\u001b[0;31m             \u001b[0;34m[\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mto_concat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         )\n\u001b[1;32m    365\u001b[0m         return self.make_block_same_class(\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def format_topics_sentences(ldamodel=lda_model, corpus=bow_corpus, texts=processed_docs):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "    \n",
    "    tic1 = datetime.now()\n",
    "    size = len(ldamodel[corpus])\n",
    "    \n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):    \n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution(% contrib) and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "        if i % 10000 == 0:\n",
    "            progress = round((i/size)*100, 3)\n",
    "            print(\"{}% completed in time {}\".format(progress, datetime.now()-tic1))\n",
    "                \n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "tic1 = datetime.now()\n",
    "print(\"Time of issuing command:{}\".format(tic1))\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=bow_corpus, texts=processed_docs)\n",
    "\n",
    "tic1 = datetime.now()\n",
    "print(\"Time of completion of command:{}\".format(tic1))\n",
    "\n",
    "# # Format\n",
    "# df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "# df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# # Show\n",
    "# df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.4199925), (1, 0.020004254), (2, 0.020004254), (3, 0.020004254), (4, 0.020004254), (5, 0.020004254), (6, 0.21994638), (7, 0.22003132), (8, 0.020004254), (9, 0.020004254)]\n",
      "[(0, 0.4199925), (7, 0.22003132), (6, 0.21994638), (1, 0.020004254), (2, 0.020004254), (3, 0.020004254), (4, 0.020004254), (5, 0.020004254), (8, 0.020004254), (9, 0.020004254)]\n",
      "[0, ('hous', 0.021601964), 0.42, 'hous, south, canberra, north, bushfir, miss, interview, hospit, investig, search']\n"
     ]
    }
   ],
   "source": [
    "row = lda_model[bow_corpus][0]\n",
    "print(row)\n",
    "# for i in bow_corpus[0]:\n",
    "#     print(dictionary[i[0]], i[1])\n",
    "row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "print(row)\n",
    "# Get the Dominant topic, Perc Contribution(% contrib) and Keywords for each document\n",
    "for j, (topic_num, prop_topic) in enumerate(row):\n",
    "    if j == 0:  # => dominant topic\n",
    "        wp = lda_model.show_topic(topic_num)\n",
    "        topic_keywords = \", \".join([word for word, prop in wp])\n",
    "        print([int(topic_num), wp[int(topic_num)], round(prop_topic,4), topic_keywords])\n",
    "#         sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "testing on an unseen document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unseen_document = 'How a Pentagon deal became an identity crisis for Google'\n",
    "print(preprocess(unseen_document), \"\\n\\n\\n\")\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_cpu] *",
   "language": "python",
   "name": "conda-env-tensorflow_cpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
